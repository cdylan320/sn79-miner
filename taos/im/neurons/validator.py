# SPDX-FileCopyrightText: 2025 Rayleigh Research <to@rayleigh.re>
# SPDX-License-Identifier: MIT
# The MIT License (MIT)

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the “Software”), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies or substantial portions of
# the Software.

# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

if __name__ != "__mp_main__":
    import os
    import json
    import signal
    import sys
    import platform
    import time
    import argparse
    import torch
    import traceback
    import xml.etree.ElementTree as ET
    import msgspec
    import math
    import shutil
    import zipfile
    import asyncio
    import posix_ipc
    import mmap
    import msgpack
    import atexit
    import multiprocessing
    import subprocess
    import struct
    from datetime import datetime, timedelta
    from ypyjson import YpyObject

    import bittensor as bt

    import uvicorn
    from typing import Tuple, Dict, List
    from fastapi import FastAPI, APIRouter
    from fastapi import Request
    import threading
    from threading import Thread, Lock, Event
    from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
    from collections import deque, defaultdict
    import numpy as np

    import psutil
    from git import Repo
    from pathlib import Path

    from taos import __spec_version__
    from taos.common.neurons.validator import BaseValidatorNeuron
    from taos.im.utils import duration_from_timestamp
    from taos.im.utils.save import save_state_worker
    from taos.im.utils.reward import get_inventory_value
    from taos.im.utils.affinity import get_core_allocation

    from taos.im.config import add_im_validator_args
    from taos.im.protocol.simulator import SimulatorResponseBatch
    from taos.im.protocol import MarketSimulationStateUpdate, FinanceEventNotification, FinanceAgentResponse
    from taos.im.protocol.models import MarketSimulationConfig, TradeInfo
    from taos.im.protocol.events import SimulationStartEvent, TradeEvent

    class Validator(BaseValidatorNeuron):
        """
        Intelligent market simulation validator implementation.

        The validator is run as a FastAPI client in order to receive messages from the simulator engine for processing and forwarding to miners.
        Metagraph maintenance, weight setting, state persistence and other general bittensor routines are executed in a separate thread.
        The validator also handles publishing of metrics via Prometheus for visualization and analysis, as well as retrieval and recording of seed data for simulation price process generation.
        """

        @classmethod
        def add_args(cls, parser: argparse.ArgumentParser) -> None:
            """
            Registers Intelligent-Markets-specific CLI configuration parameters.

            Args:
                parser (argparse.ArgumentParser): The main argument parser to extend.

            Returns:
                None
            """
            add_im_validator_args(cls, parser)

        def _setup_signal_handlers(self):
            """
            Registers OS signal handlers for graceful shutdown.

            Behavior:
                - Captures SIGINT, SIGTERM, and SIGHUP (if available).
                - Logs the received signal.
                - Triggers full validator cleanup.
                - Exits the process cleanly.

            Returns:
                None
            """
            def signal_handler(signum, frame):
                signal_name = signal.Signals(signum).name
                bt.logging.info(f"Received {signal_name}, initiating graceful shutdown...")
                self.cleanup()
                sys.exit(0)
            for sig in (signal.SIGINT, signal.SIGTERM):
                signal.signal(sig, signal_handler)
            if hasattr(signal, 'SIGHUP'):
                signal.signal(signal.SIGHUP, signal_handler)

        def _start_query_service(self):
            """
            Launches the validator's query service and initializes POSIX IPC resources.

            Responsibilities:
                - Spawns the query subprocess with correct wallet and network parameters.
                - Waits for creation of shared memory blocks and message queues.
                - Initializes memory maps for request/response communication.
                - Verifies that the query service is alive during startup.
                - Raises a RuntimeError if IPC initialization fails or service dies.

            Returns:
                None

            Raises:
                RuntimeError: If IPC endpoints are not ready within timeout
                            or if the subprocess exits unexpectedly.
            """

            bt.logging.info(f"Starting query service from: ../validator/query.py")

            core_allocation = get_core_allocation()
            cmd = [
                sys.executable,
                '-u',
                '../validator/query.py',
                '--logging.trace' if self.config.logging.trace else '--logging.debug' if self.config.logging.debug else '--logging.info',
                '--wallet.path', self.config.wallet.path,
                '--wallet.name', self.config.wallet.name,
                '--wallet.hotkey', self.config.wallet.hotkey,
                '--subtensor.network', self.config.subtensor.network,
                '--netuid', str(self.config.netuid),
                '--neuron.timeout', str(self.config.neuron.timeout),
                '--neuron.global_query_timeout', str(self.config.neuron.global_query_timeout),
                '--compression.level', str(self.config.compression.level),
                '--compression.engine', self.config.compression.engine,
                '--compression.parallel_workers', str(self.config.compression.parallel_workers),
                '--cpu-cores', ','.join(map(str, core_allocation['query'])),
            ]

            self.query_process = subprocess.Popen(cmd, stderr=sys.stderr)
            bt.logging.info(f"Query service PID: {self.query_process.pid}")

            # Wait for IPC resources to be created with retry
            queue_name = f"/validator_query_{self.config.wallet.hotkey}"

            bt.logging.info("Waiting for query service IPC resources...")
            max_retries = 30  # 30 seconds max
            for attempt in range(max_retries):
                try:
                    self.request_queue = posix_ipc.MessageQueue(f"{queue_name}_req")
                    self.response_queue = posix_ipc.MessageQueue(f"{queue_name}_res")
                    self.request_shm = posix_ipc.SharedMemory(f"{queue_name}_req_shm")
                    self.response_shm = posix_ipc.SharedMemory(f"{queue_name}_res_shm")

                    self.request_mem = mmap.mmap(self.request_shm.fd, self.request_shm.size)
                    self.response_mem = mmap.mmap(self.response_shm.fd, self.response_shm.size)

                    bt.logging.info(f"Query service ready (request_shm: {self.request_shm.size / 1024 / 1024:.0f}MB, response_shm: {self.response_shm.size / 1024 / 1024:.0f}MB)")
                    return

                except posix_ipc.ExistentialError:
                    if attempt == 0:
                        bt.logging.debug("IPC resources not ready yet, waiting...")
                    time.sleep(1)

                    # Check if process died
                    if self.query_process.poll() is not None:
                        raise RuntimeError(f"Query service died with exit code {self.query_process.returncode}")

            raise RuntimeError("Timeout waiting for query service IPC resources")

        def _start_reporting_service(self):
            bt.logging.info(f"Starting reporting service from: ../validator/report.py")

            self._reporting = False
            core_allocation = get_core_allocation()
            cmd = [
                sys.executable,
                '-u',
                '../validator/report.py',
                '--logging.trace' if self.config.logging.trace else '--logging.debug' if self.config.logging.debug else '--logging.info',
                '--wallet.path', self.config.wallet.path,
                '--wallet.name', self.config.wallet.name,
                '--wallet.hotkey', self.config.wallet.hotkey,
                '--subtensor.network', self.config.subtensor.network,
                '--netuid', str(self.config.netuid),
                '--prometheus.port', str(self.config.prometheus.port),
                '--prometheus.level', str(self.config.prometheus.level),
                '--cpu-cores', ','.join(map(str, core_allocation['reporting'])),
            ]

            self.reporting_process = subprocess.Popen(cmd, stderr=sys.stderr)
            bt.logging.info(f"Reporting service PID: {self.reporting_process.pid}")

            bt.logging.info("Waiting for reporting service IPC resources...")
            max_retries = 30
            for attempt in range(max_retries):
                try:
                    self.reporting_request_queue = posix_ipc.MessageQueue("/validator-report-req")
                    self.reporting_response_queue = posix_ipc.MessageQueue("/validator-report-res")
                    self.reporting_request_shm = posix_ipc.SharedMemory("/validator-report-data")
                    self.reporting_response_shm = posix_ipc.SharedMemory("/validator-report-response-data")

                    self.reporting_request_mem = mmap.mmap(self.reporting_request_shm.fd, self.reporting_request_shm.size)
                    self.reporting_response_mem = mmap.mmap(self.reporting_response_shm.fd, self.reporting_response_shm.size)

                    bt.logging.info(f"Reporting service ready (shm: {self.reporting_request_shm.size / 1024 / 1024:.0f}MB)")
                    return

                except posix_ipc.ExistentialError:
                    if attempt == 0:
                        bt.logging.debug("IPC resources not ready yet, waiting...")
                    time.sleep(1)

                    if self.reporting_process.poll() is not None:
                        raise RuntimeError(f"Reporting service died with exit code {self.reporting_process.returncode}")

            raise RuntimeError("Timeout waiting for reporting service IPC resources")

        def monitor(self) -> None:
            """
            Periodically checks simulator health and restarts if needed.

            Runs in a blocking loop:
                - Sleeps 5 minutes between checks.
                - Logs simulator availability.
                - Handles and logs unexpected exceptions.

            Returns:
                None
            """
            while True:
                try:
                    time.sleep(300)
                    bt.logging.info(f"Checking simulator state...")
                    if not check_simulator(self):
                        restart_simulator(self)
                    else:
                        bt.logging.info(f"Simulator online!")
                except Exception as ex:
                    bt.logging.error(f"Failure in simulator monitor : {traceback.format_exc()}")

        def seed(self) -> None:
            """
            Generates simulator seed data.

            Returns:
                None
            """
            from taos.im.validator.seed import seed
            seed(self)

        def update_repo(self, end=False) -> bool:
            """
            Checks for source or config changes in the repository and reloads components.

            Behavior:
                - Pulls latest remote changes.
                - Rebuilds simulator when C++ or Python sources change.
                - Restarts simulator on configuration changes.
                - Updates validator process when its own Python source changes.
                - Handles special behavior on simulation end.

            Args:
                end (bool): Whether the update is performed during simulation shutdown.

            Returns:
                bool: True if update steps completed successfully, False on error.
            """
            try:
                validator_py_files_changed, simulator_config_changed, simulator_py_files_changed, simulator_cpp_files_changed = check_repo(self)
                remote = self.repo.remotes[self.config.repo.remote]

                if not end:
                    if validator_py_files_changed and not (simulator_cpp_files_changed or simulator_py_files_changed):
                        bt.logging.warning("VALIDATOR LOGIC UPDATED - PULLING AND DEPLOYING.")
                        remote.pull()
                        update_validator(self)
                else:
                    try:
                        remote.pull()
                    except Exception as ex:
                        self.pagerduty_alert(f"Failed to pull changes from repo on simulation end : {ex}")
                    if simulator_cpp_files_changed or simulator_py_files_changed:
                        bt.logging.warning("SIMULATOR SOURCE CHANGED")
                        rebuild_simulator(self)
                    if simulator_config_changed:
                        bt.logging.warning("SIMULATOR CONFIG CHANGED")
                    restart_simulator(self)
                    if validator_py_files_changed:
                        update_validator(self)
                return True
            except Exception as ex:
                self.pagerduty_alert(f"Failed to update repo : {ex}", details={"traceback" : traceback.format_exc()})
                return False

        def _compress_outputs(self,  start=False):
            """
            Compresses old simulator log outputs and performs disk cleanup.

            Responsibilities:
                - Groups historical .log files into ZIP archives.
                - Removes original log files once archived.
                - Enforces storage retention when disk usage exceeds 85%.
                - Deletes dated archives and directories as needed.
                - Handles exceptions gracefully with PagerDuty reporting.

            Args:
                start (bool): If True, performs cleanup of prior simulation logs
                    even if timestamps overlap with the new run.

            Returns:
                None
            """
            self.compressing = True
            try:
                if self.simulation.logDir:
                    log_root = Path(self.simulation.logDir).parent
                    for output_dir in log_root.iterdir():
                        if output_dir.is_dir():
                            log_archives = {}
                            log_path = Path(output_dir)
                            for log_file in log_path.iterdir():
                                if log_file.is_file() and log_file.suffix == '.log':
                                    log_period = log_file.name.split('.')[1]
                                    if len(log_period) == 13:
                                        log_end = (int(log_period.split('-')[1][:2]) * 3600 + int(log_period.split('-')[1][2:4]) * 60 + int(log_period.split('-')[1][4:])) * 1_000_000_000
                                    else:
                                        log_end = (int(log_period.split('-')[1][:2]) * 86400 + int(log_period.split('-')[1][2:4]) * 3600 + int(log_period.split('-')[1][4:6]) * 60 + int(log_period.split('-')[1][6:])) * 1_000_000_000
                                    if log_end < self.simulation_timestamp or (start and str(output_dir.resolve()) != self.simulation.logDir):
                                        log_type = log_file.name.split('-')[0]
                                        label = f"{log_type}_{log_period}"
                                        if not label in log_archives:
                                            log_archives[label] = []
                                        log_archives[label].append(log_file)
                            for label, log_files in log_archives.items():
                                archive = log_path / f"{label}.zip"
                                bt.logging.info(f"Compressing {label} files to {archive.name}...")
                                with zipfile.ZipFile(archive, "w" if not archive.exists() else "a", compression=zipfile.ZIP_DEFLATED) as zipf:
                                    for log_file in log_files:
                                        try:
                                            zipf.write(log_file, arcname=Path(log_file).name)
                                            os.remove(log_file)
                                            bt.logging.debug(f"Added {log_file.name} to {archive.name}")
                                        except Exception as ex:
                                            bt.logging.error(f"Failed to add {log_file.name} to {archive.name} : {ex}")
                    if psutil.disk_usage('/').percent > 85:
                        min_retention_date = int((datetime.today() - timedelta(days=7)).strftime("%Y%m%d"))
                        bt.logging.warning(f"Disk usage > 85% - cleaning up old outputs...")
                        for output in sorted(log_root.iterdir(), key=lambda f: f.name[:13]):
                            try:
                                archive_date = int(output.name[:8])
                            except:
                                continue
                            if archive_date < min_retention_date:
                                try:
                                    if output.is_file() and output.name.endswith('.zip'):
                                        output.unlink()
                                    elif output.is_dir():
                                        shutil.rmtree(output)
                                    disk_usage = psutil.disk_usage('/').percent
                                    bt.logging.success(f"Deleted {output.name} ({disk_usage}% disk available).")
                                    if disk_usage <= 85:
                                        break
                                except Exception as ex:
                                    self.pagerduty_alert(f"Failed to remove output {output.name} : {ex}", details={"trace" : traceback.format_exc()})


            except Exception as ex:
                self.pagerduty_alert(f"Failure during output compression : {ex}", details={"trace" : traceback.format_exc()})
            finally:
                self.compressing = False

        def compress_outputs(self, start=False):
            """
            Launches asynchronous log compression in a background thread.

            Behavior:
                - Ensures only one compression job runs at a time.
                - Spawns a daemon thread to execute `_compress_outputs()`.

            Args:
                start (bool): If True, forces compression of pre-run logs.

            Returns:
                None
            """
            if not self.compressing:
                Thread(target=self._compress_outputs, args=(start,), daemon=True, name=f'compress_{self.step}').start()

        def load_simulation_config(self) -> None:
            """
            Loads the market-simulation configuration from its XML definition.

            Responsibilities:
                - Parses the XML file into a MarketSimulationConfig object.
                - Initializes paths for validator and simulation state files.
                - Loads the previous saved state (if any).

            Returns:
                None
            """
            self.xml_config = ET.parse(self.config.simulation.xml_config).getroot()
            self.simulation = MarketSimulationConfig.from_xml(self.xml_config)
            self.validator_state_file = self.config.neuron.full_path + f"/validator.mp"
            self.simulation_state_file = self.config.neuron.full_path + f"/{self.simulation.label()}.mp"
            self.load_state()

        def __init__(self, config=None) -> None:
            """
            Initializes the Intelligent Markets validator node.

            Responsibilities:
                - Loads simulation configuration from XML.
                - Initializes metagraph and subnet info.
                - Sets up executors, event loops, state locks, and signal handlers.
                - Loads prior simulation/validator state if available.
                - Initializes metrics, reporting, and query service.
                - Starts IPC-backed query subprocess.

            Args:
                config: Validator configuration object.

            Raises:
                Exception: If the simulation config XML file is missing.
            """
            super(Validator, self).__init__(config=config)
            # Load the simulator config XML file data in order to make context and parameters accessible for reporting and output location.

            if not os.path.exists(self.config.simulation.xml_config):
                raise Exception(f"Simulator config does not exist at {self.config.simulation.xml_config}!")
            self.simulator_config_file = os.path.realpath(Path(self.config.simulation.xml_config))
            # Initialize subnet info and other basic validator/simulation properties
            self.subnet_info = self.subtensor.get_metagraph_info(self.config.netuid)
            self.last_state = None
            self.last_response = None
            self.msgpack_error_counter = 0
            self.simulation_timestamp = 0
            self.reward_weights = {"sharpe" : self.config.scoring.sharpe.unrealized_weight, "sharpe_realized": self.config.scoring.sharpe.realized_weight}
            self.start_time = None
            self.start_timestamp = None
            self.last_state_time = None
            self.step_rates = []

            self.main_loop = asyncio.new_event_loop()
            self._main_loop_ready = Event()
            core_allocation = get_core_allocation()
            validator_cores = core_allocation['validator']
            os.sched_setaffinity(0, set(validator_cores))
            bt.logging.info(f"Validator assigned to cores: {validator_cores}")
            reward_cores = core_allocation['reward']
            self.reward_executor = ProcessPoolExecutor(max_workers=len(reward_cores),initializer=lambda: os.sched_setaffinity(0, set(reward_cores)))
            bt.logging.info(f"Reward executor assigned to cores: {reward_cores}")
            self.save_state_executor = ThreadPoolExecutor(max_workers=1)
            self.maintenance_executor = ThreadPoolExecutor(max_workers=1)

            self.maintaining = False
            self.compressing = False
            self.querying = False
            self._rewarding = False
            self._saving = False
            self._reporting = False
            self._rewarding_lock = Lock()
            self._saving_lock = Lock()
            self._reporting_lock = Lock()
            self._setup_signal_handlers()
            self._cleanup_done = False
            atexit.register(self.cleanup)

            self.initial_balances_published = {uid : False for uid in range(self.subnet_info.max_uids)}
            self.volume_sums = defaultdict(lambda: defaultdict(float))
            self.maker_volume_sums = defaultdict(lambda: defaultdict(float))
            self.taker_volume_sums = defaultdict(lambda: defaultdict(float))
            self.self_volume_sums = defaultdict(lambda: defaultdict(float))
            self.open_positions = defaultdict(lambda: defaultdict(lambda: {
                'longs': deque(),
                'shorts': deque()
            }))
            self.realized_pnl_history = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
            self.roundtrip_volumes = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
            self.roundtrip_volume_sums = defaultdict(lambda: defaultdict(float))

            self.load_simulation_config()

            self.router = APIRouter()
            self.router.add_api_route("/orderbook", self.orderbook, methods=["GET"])
            self.router.add_api_route("/account", self.account, methods=["GET"])

            self.repo_path = Path(os.path.dirname(os.path.realpath(__file__))).parent.parent.parent
            self.repo = Repo(self.repo_path)
            self.update_repo()

            self.miner_stats = {uid : {'requests' : 0, 'timeouts' : 0, 'failures' : 0, 'rejections' : 0, 'call_time' : []} for uid in range(self.subnet_info.max_uids)}
            self.query_process = None
            self._start_query_service()
            self.report_process = None
            self._start_reporting_service()

        def __enter__(self):
            """
            Enables use of Validator as a context manager.

            Returns:
                Validator: The active validator instance.
            """
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            """
            Ensures cleanup is triggered when exiting a context manager block.

            Args:
                exc_type: Exception type, if any.
                exc_val: Exception instance, if any.
                exc_tb: Traceback, if any.

            Returns:
                bool: False to propagate exceptions.
            """
            self.cleanup()
            return False

        @property
        def shared_state_rewarding(self):
            with self._rewarding_lock:
                return self._rewarding

        @shared_state_rewarding.setter
        def shared_state_rewarding(self, value):
            with self._rewarding_lock:
                self._rewarding = value

        @property
        def shared_state_saving(self):
            with self._saving_lock:
                return self._saving

        @shared_state_saving.setter
        def shared_state_saving(self, value):
            with self._saving_lock:
                self._saving = value

        @property
        def shared_state_reporting(self):
            with self._reporting_lock:
                return self._reporting

        @shared_state_reporting.setter
        def shared_state_reporting(self, value):
            with self._reporting_lock:
                self._reporting = value

        async def wait_for(self, check_fn: callable, message: str, interval: float = 0.01):
            """
            Asynchronously waits for a condition to become False.

            Behavior:
                - Logs a message once per second while waiting.
                - Returns immediately if condition is already False.
                - Provides debug timing on completion.

            Args:
                check_fn (callable): Function returning a boolean condition.
                message (str): Log message describing the wait condition.
                interval (float): Interval between checks in seconds.

            Returns:
                None
            """
            import time

            if not check_fn():
                return

            start_time = time.time()
            last_log_time = start_time

            bt.logging.info(message)

            while check_fn():
                await asyncio.sleep(interval)

                current_time = time.time()
                elapsed = current_time - start_time

                if current_time - last_log_time >= 1.0:
                    bt.logging.info(f"{message} (waited {elapsed:.1f}s)")
                    last_log_time = current_time

            total_wait = time.time() - start_time
            bt.logging.debug(f"Wait completed after {total_wait:.1f}s")

        async def wait_for_event(self, event: asyncio.Event, wait_process: str, run_process: str):
            """
            Waits for an asyncio.Event to be set before continuing execution.

            Provides periodic logging while waiting, and measures the total wait
            duration for operational visibility.

            Args:
                event (asyncio.Event): The event that must be completed.
                wait_process (str): Name of the process being waited on.
                run_process (str): Name of the process to run afterward.

            Returns:
                None
            """
            if not event.is_set():
                bt.logging.debug(f"Waiting for {wait_process} to complete before {run_process}...")
                start_wait = time.time()
                while not event.is_set():
                    try:
                        await asyncio.wait_for(event.wait(), timeout=1.0)
                        break
                    except asyncio.TimeoutError:
                        await asyncio.sleep(0)
                        elapsed = time.time() - start_wait
                        if int(elapsed) % 5 == 0 and elapsed > 0:
                            bt.logging.debug(f"Still waiting for {wait_process}... ({elapsed:.1f}s)")
                total_wait = time.time() - start_wait
                bt.logging.debug(f"Waited {total_wait:.1f}s for {wait_process}")

        def load_fundamental(self):
            """
            Loads fundamental price data from simulation output files.

            Behavior:
                - Reads per-block fundamental CSV files from the simulation log directory.
                - Extracts the latest fundamental price for each book ID.
                - Falls back to None for each book if no directory exists.
                - Stores results in `self.fundamental_price`.

            Returns:
                None
            """
            if self.simulation.logDir:
                prices = {}
                for block in range(self.simulation.block_count):
                    block_file = os.path.join(self.simulation.logDir, f'fundamental.{block * self.simulation.books_per_block}-{self.simulation.books_per_block * (block + 1) - 1}.csv')
                    fp_line = None
                    book_ids = None
                    for line in open(block_file, 'r').readlines():
                        if not book_ids:
                            book_ids = [int(col) for col in line.split(',') if col != "Timestamp\n"]
                        if line.strip() != '':
                            fp_line = line
                    prices = prices | {book_ids[i] : float(price) for i, price in enumerate(fp_line.strip().split(',')[:-1])}
            else:
                prices = {bookId : None for bookId in range(self.simulation.book_count)}
            self.fundamental_price = prices

        def onStart(self, timestamp, event : SimulationStartEvent) -> None:
            """
            Handles the simulator start event.

            Responsibilities:
                - Reloads simulation configuration.
                - Shifts timestamps for trade volumes, inventory, realized P&L, and round-trip volumes
                - Recalculates all volume sums to ensure consistency
                - Records simulation start time and timestamp.
                - Initializes output directory and launches log compression.
                - Loads fundamental prices for all books.
                - Resets initial balances and recent trade structures.
                - Clears open positions (can't carry over between simulations)
                - Saves initial state.

            Args:
                timestamp (int): Simulation start timestamp.
                event (SimulationStartEvent): Contains simulation log directory and metadata.

            Returns:
                None
            """
            self.load_simulation_config()
            volume_decimals = self.simulation.volumeDecimals

            bt.logging.info("Shifting timestamps and recalculating volume sums for simulation restart...")

            self.trade_volumes = {
                uid : {
                    bookId : {
                        role : {
                            prev_time - self.simulation_timestamp : volume
                            for prev_time, volume in self.trade_volumes[uid][bookId][role].items()
                            if prev_time - self.simulation_timestamp < self.simulation_timestamp
                        } for role in self.trade_volumes[uid][bookId]
                    } for bookId in range(self.simulation.book_count)
                } for uid in range(self.subnet_info.max_uids)
            }


            bt.logging.info("Recalculating trade volume sums after timestamp shift...")
            self.volume_sums = defaultdict(lambda: defaultdict(float))
            self.maker_volume_sums = defaultdict(lambda: defaultdict(float))
            self.taker_volume_sums = defaultdict(lambda: defaultdict(float))
            self.self_volume_sums = defaultdict(lambda: defaultdict(float))

            for uid in range(self.subnet_info.max_uids):
                for bookId in range(self.simulation.book_count):
                    if uid in self.trade_volumes and bookId in self.trade_volumes[uid]:
                        book_volumes = self.trade_volumes[uid][bookId]

                        # Recalculate sums
                        total_volume = sum(book_volumes['total'].values())
                        if total_volume > 0:
                            self.volume_sums[uid][bookId] = round(total_volume, volume_decimals)

                        maker_volume = sum(book_volumes['maker'].values())
                        if maker_volume > 0:
                            self.maker_volume_sums[uid][bookId] = round(maker_volume, volume_decimals)

                        taker_volume = sum(book_volumes['taker'].values())
                        if taker_volume > 0:
                            self.taker_volume_sums[uid][bookId] = round(taker_volume, volume_decimals)

                        self_volume = sum(book_volumes['self'].values())
                        if self_volume > 0:
                            self.self_volume_sums[uid][bookId] = round(self_volume, volume_decimals)

            bt.logging.info(f"Recalculated trade volume sums: {len(self.volume_sums)} total entries")

            self.inventory_history = {
                uid : {
                    prev_time - self.simulation_timestamp : values
                    for prev_time, values in self.inventory_history[uid].items()
                    if prev_time - self.simulation_timestamp < self.simulation_timestamp
                } for uid in range(self.subnet_info.max_uids)
            }

            self.start_time = time.time()
            self.simulation_timestamp = timestamp
            self.start_timestamp = self.simulation_timestamp
            self.last_state_time = None
            self.step_rates = []
            self.simulation.logDir = event.logDir
            self.compress_outputs(start=True)

            bt.logging.info("-"*40)
            bt.logging.info("SIMULATION STARTED")
            bt.logging.info("-"*40)
            bt.logging.info(f"START TIME: {self.start_time}")
            bt.logging.info(f"TIMESTAMP : {self.start_timestamp}")
            bt.logging.info(f"OUT DIR   : {self.simulation.logDir}")
            bt.logging.info("-"*40)

            self.load_fundamental()
            self.initial_balances = {
                uid : {
                    bookId : {'BASE' : None, 'QUOTE' : None, 'WEALTH' : self.simulation.miner_wealth}
                    for bookId in range(self.simulation.book_count)
                } for uid in range(self.subnet_info.max_uids)
            }
            self.recent_trades = {bookId : [] for bookId in range(self.simulation.book_count)}
            self.recent_miner_trades = {
                uid : {bookId : [] for bookId in range(self.simulation.book_count)}
                for uid in range(self.subnet_info.max_uids)
            }

            self.realized_pnl_history = {
                uid: {
                    prev_time - self.simulation_timestamp: pnl_books
                    for prev_time, pnl_books in self.realized_pnl_history[uid].items()
                    if prev_time - self.simulation_timestamp < self.simulation_timestamp
                } for uid in range(self.subnet_info.max_uids)
            }

            bt.logging.info("Shifting round-trip volume timestamps...")
            self.roundtrip_volumes = {
                uid: {
                    bookId: {
                        prev_time - self.simulation_timestamp: volume
                        for prev_time, volume in self.roundtrip_volumes.get(uid, {}).get(bookId, {}).items()
                        if prev_time - self.simulation_timestamp < self.simulation_timestamp
                    } for bookId in range(self.simulation.book_count)
                } for uid in range(self.subnet_info.max_uids)
            }

            bt.logging.info("Recalculating round-trip volume sums after timestamp shift...")
            self.roundtrip_volume_sums = defaultdict(lambda: defaultdict(float))
            for uid in range(self.subnet_info.max_uids):
                for bookId in range(self.simulation.book_count):
                    if uid in self.roundtrip_volumes and bookId in self.roundtrip_volumes[uid]:
                        total_rt_volume = sum(self.roundtrip_volumes[uid][bookId].values())
                        if total_rt_volume > 0:
                            self.roundtrip_volume_sums[uid][bookId] = round(
                                total_rt_volume,
                                volume_decimals
                            )

            bt.logging.info(f"Recalculated round-trip volume sums: {len(self.roundtrip_volume_sums)} total entries")

            self.open_positions = defaultdict(lambda: defaultdict(lambda: {
                'longs': deque(),
                'shorts': deque()
            }))

            bt.logging.info("Simulation restart complete - all timestamps shifted and sums recalculated")
            self.save_state()

        def onEnd(self) -> None:
            """
            Handles the simulator end event.

            Responsibilities:
                - Logs simulation end.
                - Clears active log directory and resets fundamental price data.
                - Clears pending notices for all UIDs.
                - Saves state one final time.
                - Pulls repo updates and rebuilds simulator if needed.

            Returns:
                None
            """
            bt.logging.info("SIMULATION ENDED")
            self.simulation.logDir = None
            self.fundamental_price = {bookId : None for bookId in range(self.simulation.book_count)}
            self.pending_notices = {uid : [] for uid in range(self.subnet_info.max_uids)}
            self.save_state()
            self.update_repo(end=True)

        def _construct_save_data(self):
            """
            Builds the simulation-state and validator-state dictionaries
            required for serialization.

            Includes:
                - Simulation timing, balances, trade histories, notices, and log paths.
                - Validator scoring, volumes, activity factors, and metadata.

            Returns:
                tuple(dict, dict): (simulation_state_data, validator_state_data)
            """
            start = time.time()
            bt.logging.debug("Preparing state for saving...")
            simulation_state_data = {
                "start_time": self.start_time,
                "start_timestamp": self.start_timestamp,
                "step_rates": self.step_rates,
                "initial_balances": self.initial_balances,
                "recent_trades": {
                    book_id: [t.model_dump(mode="json") for t in trades]
                    for book_id, trades in self.recent_trades.items()
                },
                "recent_miner_trades": {
                    uid: {
                        book_id: [[t.model_dump(mode="json"), r] for t, r in trades]
                        for book_id, trades in uid_trades.items()
                    }
                    for uid, uid_trades in self.recent_miner_trades.items()
                },
                "pending_notices": self.pending_notices,
                "simulation.logDir": self.simulation.logDir,
            }

            def nested_dict_to_regular(d):
                """Convert nested defaultdict to regular dict for serialization."""
                return {
                    uid: dict(books) for uid, books in d.items()
                }

            validator_state_data = {
                "step": self.step,
                "simulation_timestamp": self.simulation_timestamp,
                "hotkeys": self.hotkeys,
                "scores": [score.item() for score in self.scores],
                "activity_factors": self.activity_factors,
                "activity_factors_realized": self.activity_factors_realized,
                "inventory_history": self.inventory_history,
                "sharpe_values": self.sharpe_values,
                "realized_pnl_history": self.realized_pnl_history,
                "open_positions": {
                    uid: {
                        book_id: {
                            'longs': list(pos['longs']),
                            'shorts': list(pos['shorts'])
                        }
                        for book_id, pos in books.items()
                    }
                    for uid, books in self.open_positions.items()
                },
                "unnormalized_scores": self.unnormalized_scores,
                "deregistered_uids": self.deregistered_uids,
                "trade_volumes": self.trade_volumes,
                "roundtrip_volumes": {
                    uid: {
                        book_id: dict(volumes)
                        for book_id, volumes in books.items()
                    }
                    for uid, books in self.roundtrip_volumes.items()
                },
                "volume_sums": nested_dict_to_regular(self.volume_sums),
                "maker_volume_sums": nested_dict_to_regular(self.maker_volume_sums),
                "taker_volume_sums": nested_dict_to_regular(self.taker_volume_sums),
                "self_volume_sums": nested_dict_to_regular(self.self_volume_sums),
                "roundtrip_volume_sums": nested_dict_to_regular(self.roundtrip_volume_sums)
            }
            bt.logging.debug(f"Prepared save data ({time.time() - start}s)")
            return simulation_state_data, validator_state_data

        async def _save_state(self) -> bool:
            """
            Saves simulation and validator state asynchronously via executor workers.

            Behavior:
                - Prevents concurrent saves via shared-state lock.
                - Offloads write operations to a separate process.
                - Logs performance metrics and error details.
                - Raises PagerDuty alerts on failure.

            Returns:
                bool: True if state saved successfully, False otherwise.
            """
            if self.shared_state_saving:
                bt.logging.warning(f"Skipping save at step {self.step} — previous save still running.")
                return False

            self.shared_state_saving = True

            try:
                bt.logging.info(f"Starting state saving for step {self.step}...")
                start = time.time()
                simulation_state_data, validator_state_data = self._construct_save_data()
                bt.logging.debug("Saving state...")
                future_start = time.time()

                future = asyncio.get_running_loop().run_in_executor(
                    self.save_state_executor,
                    save_state_worker,
                    simulation_state_data,
                    validator_state_data,
                    self.simulation_state_file,
                    self.validator_state_file
                )
                while not future.done():
                    await asyncio.sleep(0.1)
                result = future.result()
                bt.logging.debug(f"Saved state ({time.time() - future_start}s)")

                if result['success']:
                    bt.logging.success(
                        f"Simulation state saved to {self.simulation_state_file} "
                        f"({result['simulation_save_time']:.4f}s)"
                    )
                    bt.logging.success(
                        f"Validator state saved to {self.validator_state_file} "
                        f"({result['validator_save_time']:.4f}s)"
                    )
                    bt.logging.info(f"Total save time: {result['total_time']:.4f}s | {time.time()-start}s")
                    return True
                else:
                    bt.logging.error(f"Failed to save state: {result['error']}")
                    if result.get('traceback'):
                        bt.logging.debug(result['traceback'])
                    self.pagerduty_alert(
                        f"Failed to save state: {result['error']}",
                        details={"trace": result.get('traceback')}
                    )
                    return False

            except Exception as ex:
                bt.logging.error(f"Error preparing state for save: {ex}")
                bt.logging.debug(traceback.format_exc())
                self.pagerduty_alert(
                    f"Failed to prepare state for save: {ex}",
                    details={"trace": traceback.format_exc()}
                )
                return False
            finally:
                self.shared_state_saving = False

        def save_state(self) -> None:
            """
            Schedules the asynchronous state-saving coroutine on the main event loop.

            Behavior:
                - Executes only at specific scoring intervals.
                - Ensures no previous save task is still running.
                - Dispatches `_save_state()` thread-safely from the maintenance thread.

            Returns:
                None
            """
            if not self.last_state or self.last_state.timestamp % self.config.scoring.interval != 4_000_000_000:
                return
            if self.shared_state_saving:
                bt.logging.warning(f"Skipping save at step {self.step} — previous save still running.")
                return
            bt.logging.debug(f"[SAVE] Scheduling from thread: {threading.current_thread().name}")
            bt.logging.debug(f"[SAVE] Main loop ID: {id(self.main_loop)}, Current loop ID: {id(asyncio.get_event_loop())}")
            self.main_loop.call_soon_threadsafe(lambda: self.main_loop.create_task(self._save_state()))

        def _save_state_sync(self):
            """
            Performs a fully synchronous state save without using executors.

            Used when:
                - The event loop or executor pools have already shut down.
                - A fallback save must occur during shutdown or reorg.

            Returns:
                None
            """
            try:
                bt.logging.info("Saving state (sync)...")

                simulation_state_data, validator_state_data = self._construct_save_data()

                # Call worker function directly (synchronously in main process)
                result = save_state_worker(
                    simulation_state_data,
                    validator_state_data,
                    self.simulation_state_file,
                    self.validator_state_file
                )

                if result['success']:
                    bt.logging.success(
                        f"State saved directly: simulation ({result['simulation_save_time']:.4f}s), "
                        f"validator ({result['validator_save_time']:.4f}s)"
                    )
                else:
                    bt.logging.error(f"Direct save failed: {result['error']}")
            except Exception as ex:
                bt.logging.error(f"Error in direct save: {ex}]\n{traceback.format_exc()}")

        def load_state(self) -> None:
            """
            Loads validator and simulation state from msgpack or legacy PyTorch files.

            Behavior:
                - Converts `.pt` state files to msgpack if detected.
                - Loads simulation variables (balances, trades, notices, timestamps).
                - Loads validator data (scores, activity, Sharpe values, volumes).
                - Reconstructs missing fields or reshapes data when schema versions differ.
                - Reinitializes state when `neuron.reset=True` or files are absent.

            Returns:
                None
            """
            if os.path.exists(self.simulation_state_file.replace('.mp', '.pt')):
                bt.logging.info("Pytorch simulation state file exists - converting to msgpack...")
                pt_simulation_state = torch.load(self.simulation_state_file.replace('.mp', '.pt'), weights_only=False)
                with open(self.simulation_state_file, 'wb') as file:
                    packed_data = msgpack.packb(
                        {
                            "start_time": pt_simulation_state['start_time'],
                            "start_timestamp": pt_simulation_state['start_timestamp'],
                            "step_rates": pt_simulation_state['step_rates'],
                            "initial_balances": pt_simulation_state['initial_balances'],
                            "recent_trades": {book_id : [t.model_dump(mode='json') for t in book_trades] for book_id, book_trades in pt_simulation_state['recent_trades'].items()},
                            "recent_miner_trades": {uid : {book_id : [[t.model_dump(mode='json'), r] for t, r in trades] for book_id, trades in uid_miner_trades.items()} for uid, uid_miner_trades in pt_simulation_state['recent_miner_trades'].items()},
                            "pending_notices": pt_simulation_state['pending_notices'],
                            "simulation.logDir": pt_simulation_state['simulation.logDir']
                        }, use_bin_type=True
                    )
                    file.write(packed_data)
                os.rename(self.simulation_state_file.replace('.mp', '.pt'), self.simulation_state_file.replace('.mp', '.pt') + ".bak")
                bt.logging.info(f"Pytorch simulation state file converted to msgpack at {self.simulation_state_file}")

            if not self.config.neuron.reset and os.path.exists(self.simulation_state_file):
                bt.logging.info(f"Loading simulation state variables from {self.simulation_state_file}...")
                with open(self.simulation_state_file, 'rb') as file:
                    byte_data = file.read()
                simulation_state = msgpack.unpackb(byte_data, use_list=True, strict_map_key=False)
                self.start_time = simulation_state["start_time"]
                self.start_timestamp = simulation_state["start_timestamp"]
                self.step_rates = simulation_state["step_rates"]
                self.pending_notices = simulation_state["pending_notices"]
                self.initial_balances = simulation_state["initial_balances"] if 'initial_balances' in simulation_state else {uid : {bookId : {'BASE' : None, 'QUOTE' : None, 'WEALTH' : None} for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                for uid, initial_balances in self.initial_balances.items():
                    if not 'WEALTH' in initial_balances[0]:
                        self.initial_balances[uid] = {bookId : initial_balance | {'WEALTH' : self.simulation.miner_wealth} for bookId, initial_balance in initial_balances.items()}
                self.recent_trades = {book_id : [TradeInfo.model_construct(**t) for t in book_trades] for book_id, book_trades in simulation_state["recent_trades"].items()}
                self.recent_miner_trades = {uid : {book_id : [[TradeEvent.model_construct(**t), r] for t, r in trades] for book_id, trades in uid_miner_trades.items()} for uid, uid_miner_trades in simulation_state["recent_miner_trades"].items()}  if "recent_miner_trades" in simulation_state else {uid : {bookId : [] for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                self.simulation.logDir = simulation_state["simulation.logDir"]
                bt.logging.success(f"Loaded simulation state.")
            else:
                # If no state exists or the neuron.reset flag is set, re-initialize the simulation state
                if self.config.neuron.reset and os.path.exists(self.simulation_state_file):
                    bt.logging.warning(f"`neuron.reset is True, ignoring previous state info at {self.simulation_state_file}.")
                else:
                    bt.logging.info(f"No previous state information at {self.simulation_state_file}, initializing new simulation state.")
                self.pending_notices = {uid : [] for uid in range(self.subnet_info.max_uids)}
                self.initial_balances = {uid : {bookId : {'BASE' : None, 'QUOTE' : None, 'WEALTH' : None} for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                self.recent_trades = {bookId : [] for bookId in range(self.simulation.book_count)}
                self.recent_miner_trades = {uid : {bookId : [] for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                self.fundamental_price = {bookId : None for bookId in range(self.simulation.book_count)}

            if os.path.exists(self.validator_state_file.replace('.mp', '.pt')):
                bt.logging.info("Pytorch validator state file exists - converting to msgpack...")
                pt_validator_state = torch.load(self.validator_state_file.replace('.mp', '.pt'), weights_only=False)
                pt_validator_state["scores"] = [score.item() for score in pt_validator_state['scores']]
                with open(self.validator_state_file, 'wb') as file:
                    packed_data = msgpack.packb(
                        pt_validator_state, use_bin_type=True
                    )
                    file.write(packed_data)
                os.rename(self.validator_state_file.replace('.mp', '.pt'), self.validator_state_file.replace('.mp', '.pt') + ".bak")
                bt.logging.info(f"Pytorch validator state file converted to msgpack at {self.validator_state_file}")

            if not self.config.neuron.reset and os.path.exists(self.validator_state_file):
                bt.logging.info(f"Loading validator state variables from {self.validator_state_file}...")
                with open(self.validator_state_file, 'rb') as file:
                    byte_data = file.read()
                validator_state = msgpack.unpackb(byte_data, use_list=False, strict_map_key=False)
                self.step = validator_state["step"]
                self.simulation_timestamp = validator_state["simulation_timestamp"] if "simulation_timestamp" in validator_state else 0
                self.hotkeys = validator_state["hotkeys"]
                self.deregistered_uids = list(validator_state["deregistered_uids"]) if "deregistered_uids" in validator_state else []
                self.scores = torch.tensor(validator_state["scores"])
                self.activity_factors = validator_state["activity_factors"] if "activity_factors" in validator_state else {uid : {bookId : 0.0 for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                if isinstance(self.activity_factors[0], float):
                    self.activity_factors = {uid : {bookId : self.activity_factors[uid] for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                self.activity_factors_realized = validator_state.get("activity_factors_realized", {uid : {bookId : 0.0 for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)})
                self.inventory_history = validator_state["inventory_history"] if "inventory_history" in validator_state else {uid : {} for uid in range(self.subnet_info.max_uids)}
                for uid in self.inventory_history:
                    for timestamp in self.inventory_history[uid]:
                        if len(self.inventory_history[uid][timestamp]) < self.simulation.book_count:
                            for bookId in range(len(self.inventory_history[uid][timestamp]),self.simulation.book_count):
                                self.inventory_history[uid][timestamp][bookId] = 0.0
                        if len(self.inventory_history[uid][timestamp]) > self.simulation.book_count:
                            self.inventory_history[uid][timestamp] = {k : v for k, v in self.inventory_history[uid][timestamp].items() if k < self.simulation.book_count}
                self.sharpe_values = validator_state["sharpe_values"]
                for uid in self.sharpe_values:
                    if self.sharpe_values[uid]:
                        if 'books_realized' not in self.sharpe_values[uid]:
                            self.sharpe_values[uid]['books_realized'] = {
                                bookId: 0.0 for bookId in range(self.simulation.book_count)
                            }
                        if 'books_weighted_realized' not in self.sharpe_values[uid]:
                            self.sharpe_values[uid]['books_weighted_realized'] = {
                                bookId: 0.0 for bookId in range(self.simulation.book_count)
                            }
                        for field in ['total_realized', 'average_realized', 'median_realized',
                                    'normalized_average_realized', 'normalized_total_realized',
                                    'normalized_median_realized', 'activity_weighted_normalized_median_realized',
                                    'penalty_realized', 'score_realized']:
                            if field not in self.sharpe_values[uid]:
                                self.sharpe_values[uid][field] = 0.0
                self.unnormalized_scores = validator_state["unnormalized_scores"]
                self.trade_volumes = validator_state["trade_volumes"] if "trade_volumes" in validator_state else {uid : {bookId : {'total' : {}, 'maker' : {}, 'taker' : {}, 'self' : {}} for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                reorg = False
                for uid in self.trade_volumes:
                    for bookId in self.trade_volumes[uid]:
                        if not 'total' in self.trade_volumes[uid][bookId]:
                            if not reorg:
                                bt.logging.info(f"Optimizing miner volume history structures...")
                                reorg = True
                            volumes = {'total' : {}, 'maker' : {}, 'taker' : {}, 'self' : {}}
                            for time, role_volume in self.trade_volumes[uid][bookId].items():
                                sampled_time = math.ceil(time / self.config.scoring.activity.trade_volume_sampling_interval) * self.config.scoring.activity.trade_volume_sampling_interval
                                for role, volume in role_volume.items():
                                    if not sampled_time in volumes[role]:
                                        volumes[role][sampled_time] = 0.0
                                    volumes[role][sampled_time] += volume
                            self.trade_volumes[uid][bookId] = {role : {time : round(volumes[role][time], self.simulation.volumeDecimals) for time in volumes[role]} for role in volumes}
                    if len(self.trade_volumes[uid]) < self.simulation.book_count:
                        for bookId in range(len(self.trade_volumes[uid]),self.simulation.book_count):
                            self.trade_volumes[uid][bookId] = {'total' : {}, 'maker' : {}, 'taker' : {}, 'self' : {}}
                    if len(self.trade_volumes[uid]) > self.simulation.book_count:
                        self.trade_volumes[uid] = {k : v for k, v in self.trade_volumes[uid].items() if k < self.simulation.book_count}
                    if len(self.activity_factors[uid]) < self.simulation.book_count:
                        for bookId in range(len(self.activity_factors[uid]),self.simulation.book_count):
                            self.activity_factors[uid][bookId] = 0.0
                    if len(self.activity_factors[uid]) > self.simulation.book_count:
                        self.activity_factors[uid] = {k : v for k, v in self.activity_factors[uid].items() if k < self.simulation.book_count}
                    if len(self.activity_factors_realized[uid]) < self.simulation.book_count:
                        for bookId in range(len(self.activity_factors_realized[uid]), self.simulation.book_count):
                            self.activity_factors_realized[uid][bookId] = 0.0
                    if len(self.activity_factors_realized[uid]) > self.simulation.book_count:
                        self.activity_factors_realized[uid] = {k : v for k, v in self.activity_factors_realized[uid].items() if k < self.simulation.book_count}
                self.roundtrip_volumes = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
                if "roundtrip_volumes" in validator_state:
                    for uid, books in validator_state["roundtrip_volumes"].items():
                        for book_id, volumes in books.items():
                            self.roundtrip_volumes[uid][book_id] = defaultdict(float)
                            for timestamp, volume in volumes.items():
                                self.roundtrip_volumes[uid][book_id][timestamp] = volume

                def load_volume_sums(data, name):
                    """Load volume sums with backward compatibility for tuple keys."""
                    result = defaultdict(lambda: defaultdict(float))

                    if name not in data:
                        bt.logging.info(f"No {name} in saved state, initializing empty")
                        return result

                    volume_data = data[name]

                    # Detect format by checking first key
                    if volume_data:
                        first_key = next(iter(volume_data.keys()))

                        # Check if tuple key format: (uid, book_id)
                        if isinstance(first_key, (tuple, list)) and len(first_key) == 2:
                            bt.logging.info(f"Converting {name} from old tuple-key format to nested dict...")
                            for key, vol in volume_data.items():
                                uid, book_id = key
                                result[uid][book_id] = vol
                            bt.logging.debug(f"Converted {len(volume_data)} entries in {name}")

                        # New nested dict format: {uid: {book_id: vol}}
                        elif isinstance(first_key, int):
                            first_value = volume_data[first_key]

                            # Check if nested dict
                            if isinstance(first_value, dict):
                                bt.logging.debug(f"Loading {name} in nested dict format...")
                                for uid, books in volume_data.items():
                                    for book_id, vol in books.items():
                                        result[uid][book_id] = vol
                            else:
                                # Single level dict - shouldn't happen but handle gracefully
                                bt.logging.warning(f"Unexpected format for {name}: single-level dict")
                                result[first_key][0] = first_value
                        else:
                            bt.logging.warning(f"Unknown format for {name}, initializing empty")

                    return result

                # Load all volume sums with backward compatibility
                self.volume_sums = load_volume_sums(validator_state, "volume_sums")
                self.maker_volume_sums = load_volume_sums(validator_state, "maker_volume_sums")
                self.taker_volume_sums = load_volume_sums(validator_state, "taker_volume_sums")
                self.self_volume_sums = load_volume_sums(validator_state, "self_volume_sums")
                self.roundtrip_volume_sums = load_volume_sums(validator_state, "roundtrip_volume_sums")

                self.realized_pnl_history = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
                if "realized_pnl_history" in validator_state:
                    for uid, hist in validator_state["realized_pnl_history"].items():
                        for timestamp, books in hist.items():
                            for book_id, pnl in books.items():
                                self.realized_pnl_history[uid][timestamp][book_id] = pnl
                else:
                    bt.logging.info("No realized P&L history in saved state, initializing empty")
                self.open_positions = defaultdict(lambda: defaultdict(lambda: {
                    'longs': deque(),
                    'shorts': deque()
                }))
                if "open_positions" in validator_state:
                    bt.logging.info("Loading open positions from saved state...")
                    legacy_count = 0
                    for uid, books in validator_state["open_positions"].items():
                        for book_id, pos in books.items():
                            longs = []
                            for p in pos['longs']:
                                if len(p) == 4:
                                    longs.append(tuple(p))
                                elif len(p) == 3:
                                    longs.append((*p, 0.0))
                                    legacy_count += 1
                                else:
                                    bt.logging.warning(f"Unexpected position tuple length {len(p)} for uid {uid} book {book_id}")

                            shorts = []
                            for p in pos['shorts']:
                                if len(p) == 4:
                                    shorts.append(tuple(p))
                                elif len(p) == 3:
                                    shorts.append((*p, 0.0))
                                    legacy_count += 1
                                else:
                                    bt.logging.warning(f"Unexpected position tuple length {len(p)} for uid {uid} book {book_id}")

                            self.open_positions[uid][book_id]['longs'] = deque(longs)
                            self.open_positions[uid][book_id]['shorts'] = deque(shorts)
                    if legacy_count > 0:
                        bt.logging.info(f"Converted {legacy_count} legacy positions (3-tuple) to new format (4-tuple with 0.0 fees)")
                else:
                    bt.logging.info("No open positions in saved state, initializing empty")
                if reorg:
                    self._save_state()
                bt.logging.success(f"Loaded validator state.")
            else:
                # If no state exists or the neuron.reset flag is set, re-initialize the validator state
                if self.config.neuron.reset and os.path.exists(self.validator_state_file):
                    bt.logging.warning(f"`neuron.reset is True, ignoring previous state info at {self.validator_state_file}.")
                else:
                    bt.logging.info(f"No previous state information at {self.validator_state_file}, initializing new simulation state.")
                self.activity_factors = {uid : {bookId : 0.0 for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}
                self.inventory_history = {uid : {} for uid in range(self.subnet_info.max_uids)}
                self.sharpe_values = {uid:
                    {
                        'books': {bookId: 0.0 for bookId in range(self.simulation.book_count)},
                        'books_realized': {bookId: 0.0 for bookId in range(self.simulation.book_count)},
                        'books_weighted': {bookId: 0.0 for bookId in range(self.simulation.book_count)},
                        'books_weighted_realized': {bookId: 0.0 for bookId in range(self.simulation.book_count)},
                        'total': 0.0,
                        'total_realized': 0.0,
                        'average': 0.0,
                        'average_realized': 0.0,
                        'median': 0.0,
                        'median_realized': 0.0,
                        'normalized_average': 0.0,
                        'normalized_average_realized': 0.0,
                        'normalized_total': 0.0,
                        'normalized_total_realized': 0.0,
                        'normalized_median': 0.0,
                        'normalized_median_realized': 0.0,
                        'activity_weighted_normalized_median': 0.0,
                        'activity_weighted_normalized_median_realized': 0.0,
                        'book_balance_multipliers': {bookId: 0.0 for bookId in range(self.simulation.book_count)},
                        'balance_ratio_multiplier': 0.0,
                        'penalty': 0.0,
                        'penalty_realized': 0.0,
                        'score_unrealized': 0.0,
                        'score_realized': 0.0,
                        'score': 0.0,
                    } for uid in range(self.subnet_info.max_uids)
                }
                self.unnormalized_scores = {uid : 0.0 for uid in range(self.subnet_info.max_uids)}
                self.trade_volumes = {uid : {bookId : {'total' : {}, 'maker' : {}, 'taker' : {}, 'self' : {}} for bookId in range(self.simulation.book_count)} for uid in range(self.subnet_info.max_uids)}

        def handle_deregistration(self, uid) -> None:
            """
            Handles deregistration of a validator or miner UID.

            Behavior:
                - Flags the UID for balance/state reset.
                - Zeros current score.
                - Logs deregistration action.

            Args:
                uid (int): UID being deregistered.

            Returns:
                None
            """
            self.deregistered_uids.append(uid)
            self.scores[uid] = 0.0
            bt.logging.debug(f"UID {uid} Deregistered - Scheduled for reset.")

        def process_resets(self, state : MarketSimulationStateUpdate) -> None:
            """
            Processes reset notices delivered by the simulator.

            Behavior:
                - Detects successful agent reset events (RDRA / ERDRA).
                - Resets Sharpe values, activity factors, volume histories, inventory,
                and all accumulated metrics for each affected UID.
                - Removes UID from deregistration list after reset.
                - Restores the UID to a clean initial state.
                - Issues a PagerDuty alert if reset fails.

            Args:
                state (MarketSimulationStateUpdate): Contains notices and reset messages.

            Returns:
                None
            """
            for notice in state.notices[self.uid]:
                if notice['y'] in ["RESPONSE_DISTRIBUTED_RESET_AGENT", "RDRA"] or notice['y'] in ["ERROR_RESPONSE_DISTRIBUTED_RESET_AGENT", "ERDRA"]:
                    for reset in notice['r']:
                        if reset['u']:
                            bt.logging.info(f"Agent {reset['a']} Balances Reset! {reset}")
                            if reset['a'] in self.deregistered_uids:
                                self.sharpe_values[reset['a']] = {
                                    'books': {
                                        bookId: 0.0 for bookId in range(self.simulation.book_count)
                                    },
                                    'books_realized': {
                                        bookId: 0.0 for bookId in range(self.simulation.book_count)
                                    },
                                    'books_weighted': {
                                        bookId: 0.0 for bookId in range(self.simulation.book_count)
                                    },
                                    'books_weighted_realized': {
                                        bookId: 0.0 for bookId in range(self.simulation.book_count)
                                    },
                                    'total': 0.0,
                                    'total_realized': 0.0,
                                    'average': 0.0,
                                    'average_realized': 0.0,
                                    'median': 0.0,
                                    'median_realized': 0.0,
                                    'normalized_average': 0.0,
                                    'normalized_average_realized': 0.0,
                                    'normalized_total': 0.0,
                                    'normalized_total_realized': 0.0,
                                    'normalized_median': 0.0,
                                    'normalized_median_realized': 0.0,
                                    'activity_weighted_normalized_median': 0.0,
                                    'activity_weighted_normalized_median_realized': 0.0,
                                    'penalty': 0.0,
                                    'penalty_realized': 0.0,
                                    'book_balance_multipliers': {
                                        bookId: 0.0 for bookId in range(self.simulation.book_count)
                                    },
                                    'balance_ratio_multiplier': 0.0,
                                    'score_unrealized': 0.0,
                                    'score_realized': 0.0,
                                    'score': 0.0,
                                }
                                self.unnormalized_scores[reset['a']] = 0.0
                                self.activity_factors[reset['a']] = {bookId: 0.0 for bookId in range(self.simulation.book_count)}
                                self.activity_factors_realized[reset['a']] = {bookId: 0.0 for bookId in range(self.simulation.book_count)}
                                self.inventory_history[reset['a']] = {}
                                self.trade_volumes[reset['a']] = {bookId: {'total': {}, 'maker': {}, 'taker': {}, 'self': {}} for bookId in range(self.simulation.book_count)}

                                # Clear volume sums
                                for book_id in range(self.simulation.book_count):
                                    self.volume_sums[reset['a']][book_id] = 0.0
                                    self.maker_volume_sums[reset['a']][book_id] = 0.0
                                    self.taker_volume_sums[reset['a']][book_id] = 0.0
                                    self.self_volume_sums[reset['a']][book_id] = 0.0
                                self.roundtrip_volumes[reset['a']] = defaultdict(lambda: defaultdict(float))
                                for book_id in range(self.simulation.book_count):
                                    self.roundtrip_volume_sums[reset['a']][book_id] = 0.0

                                self.realized_pnl_history[reset['a']] = {}
                                self.open_positions[reset['a']] = defaultdict(lambda: {
                                    'longs': deque(),
                                    'shorts': deque()
                                })

                                self.initial_balances[reset['a']] = {bookId: {'BASE': None, 'QUOTE': None, 'WEALTH': None} for bookId in range(self.simulation.book_count)}
                                self.initial_balances_published[reset['a']] = False
                                self.deregistered_uids.remove(reset['a'])
                                self.miner_stats[reset['a']] = {'requests': 0, 'timeouts': 0, 'failures': 0, 'rejections': 0, 'call_time': []}
                                self.recent_miner_trades[reset['a']] = {bookId: [] for bookId in range(self.simulation.book_count)}
                        else:
                            self.pagerduty_alert(f"Failed to Reset Agent {reset['a']} : {reset['m']}")

        async def _maintain(self) -> None:
            """
            Executes metagraph sync and maintenance operations asynchronously.

            Actions:
                - Marks the validator as in maintenance mode.
                - Runs synchronous maintenance work in an executor thread.
                - Logs timing and reports issues via PagerDuty.

            Returns:
                None
            """
            try:
                self.maintaining = True
                bt.logging.info(f"Synchronizing at Step {self.step}...")
                start = time.time()
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    self.maintenance_executor,
                    self._sync_and_check
                )
                bt.logging.info(f"Synchronized ({time.time()-start:.4f}s)")

            except Exception as ex:
                self.pagerduty_alert(f"Failed to sync: {ex}", details={"trace": traceback.format_exc()})
            finally:
                self.maintaining = False

        def _sync_and_check(self):
            """
            Performs synchronous metagraph maintenance and simulator health checks.

            Steps:
                - Runs Bittensor sync (without saving state).
                - Verifies simulator health.
                - Restarts simulator if unhealthy.

            Returns:
                None
            """
            self.sync(save_state=False)
            if not check_simulator(self):
                restart_simulator(self)

        def maintain(self) -> None:
            """
            Schedules asynchronous maintenance work from the maintenance thread.

            Behavior:
                - Ensures maintenance is not already running.
                - Triggers only at specific simulation timestamps.
                - Sends a coroutine to the main event loop thread-safely.

            Returns:
                None
            """
            if not self.maintaining and self.last_state and self.last_state.timestamp % self.config.scoring.interval == 2_000_000_000:
                bt.logging.debug(f"[MAINT] Scheduling from thread: {threading.current_thread().name}")
                bt.logging.debug(f"[MAINT] Main loop ID: {id(self.main_loop)}, Current loop ID: {id(asyncio.get_event_loop())}")
                self.main_loop.call_soon_threadsafe(lambda: self.main_loop.create_task(self._maintain()))

        def _prepare_reporting_data(self):
            bt.logging.debug(f"Retrieving fundamental prices...")
            start = time.time()
            self.load_fundamental()
            bt.logging.debug(f"Retrieved fundamental prices ({time.time()-start:.4f}s).")

            def serialize_nested_dict(d):
                """Convert nested dict to flat string keys."""
                return {
                    f"{uid}:{book_id}": vol
                    for uid, books in d.items()
                    for book_id, vol in books.items()    }

            return {
                'metagraph_data': {
                    'hotkeys': [str(hk) for hk in self.metagraph.hotkeys],
                    'stake': self.metagraph.stake.tolist(),
                    'trust': self.metagraph.trust.tolist(),
                    'consensus': self.metagraph.consensus.tolist(),
                    'incentive': self.metagraph.incentive.tolist(),
                    'emission': self.metagraph.emission.tolist(),
                    'validator_trust': self.metagraph.validator_trust.tolist(),
                    'dividends': self.metagraph.dividends.tolist(),
                    'active': self.metagraph.active.tolist(),
                    'last_update': self.metagraph.last_update.tolist(),
                },
                'simulation': self.simulation.model_dump(),
                'last_state': {
                    'accounts': self.last_state.accounts,
                    'books': self.last_state.books,
                    'notices': self.last_state.notices,
                },
                'simulation_timestamp': self.simulation_timestamp,
                'step': self.step,
                'step_rates': list(self.step_rates),
                'volume_sums': serialize_nested_dict(self.volume_sums),
                'maker_volume_sums': serialize_nested_dict(self.maker_volume_sums),
                'taker_volume_sums': serialize_nested_dict(self.taker_volume_sums),
                'self_volume_sums': serialize_nested_dict(self.self_volume_sums),
                'roundtrip_volume_sums': serialize_nested_dict(self.roundtrip_volume_sums),
                'inventory_history': self.inventory_history,
                'activity_factors': self.activity_factors,
                'activity_factors_realized': self.activity_factors_realized,
                'sharpe_values': self.sharpe_values,
                'unnormalized_scores': self.unnormalized_scores,
                'scores': {i: score.item() for i, score in enumerate(self.scores)},
                'miner_stats': self.miner_stats,
                'initial_balances': self.initial_balances,
                'initial_balances_published': self.initial_balances_published,
                'recent_trades': {bookId: [t.model_dump() for t in trades] for bookId, trades in self.recent_trades.items()},
                'recent_miner_trades': {
                    uid: {
                        bookId: [{'trade': miner_trade.model_dump(), 'role': role} for miner_trade, role in trades]
                        for bookId, trades in book_trades.items()
                    }
                    for uid, book_trades in self.recent_miner_trades.items()
                },
                'realized_pnl_history': self.realized_pnl_history,
                'open_positions': {
                    uid: {
                        book_id: {
                            'longs_count': len(pos['longs']),
                            'shorts_count': len(pos['shorts'])
                        }
                        for book_id, pos in books.items()
                    }
                    for uid, books in self.open_positions.items()
                },
                'fundamental_price': self.fundamental_price,
                'shared_state_rewarding': self.shared_state_rewarding,
                'current_block': self.current_block,
                'uid': self.uid,
            }

        async def _report(self):
            if self._reporting:
                bt.logging.warning(f"Previous reporting still in progress, skipping step {self.step}")
                return

            if self.reporting_process.poll() is not None:
                bt.logging.error(f"Reporting service died with exit code {self.reporting_process.returncode}")
                bt.logging.error("Attempting to restart reporting service...")
                self._start_reporting_service()
                if self.reporting_process.poll() is not None:
                    bt.logging.error("Failed to restart reporting service")
                    return

            self._reporting = True
            reporting_step = self.step
            bt.logging.info(f"Starting Reporting at step {reporting_step}...")
            start = time.time()
            try:
                while True:
                    try:
                        await asyncio.to_thread(self.reporting_response_queue.receive, timeout=0.0)
                        bt.logging.warning("Drained stale message from reporting response queue")
                    except posix_ipc.BusyError:
                        break

                data = self._prepare_reporting_data()

                write_start = time.time()
                serialize_start = time.time()
                data_bytes = await asyncio.to_thread(msgpack.packb, data, use_bin_type=True)
                serialize_time = time.time() - serialize_start

                data_mb = len(data_bytes) / 1024 / 1024
                bt.logging.info(f"Reporting data: {data_mb:.2f} MB (serialize={serialize_time:.4f}s)")

                def write_data():
                    self.reporting_request_mem.seek(0)
                    self.reporting_request_mem.write(struct.pack('Q', len(data_bytes)))
                    self.reporting_request_mem.write(data_bytes)

                await asyncio.to_thread(write_data)
                bt.logging.info(f"Wrote reporting data ({time.time()-write_start:.4f}s)")

                receive_start = time.time()
                await asyncio.to_thread(self.reporting_request_queue.send, b'publish')
                message, _ = await asyncio.to_thread(self.reporting_response_queue.receive)
                bt.logging.info(f"Received reporting response ({time.time()-receive_start:.4f}s).")

                read_start = time.time()

                def read_response():
                    self.reporting_response_mem.seek(0)
                    size_bytes = self.reporting_response_mem.read(8)
                    data_size = struct.unpack('Q', size_bytes)[0]
                    result_bytes = self.reporting_response_mem.read(data_size)
                    return msgpack.unpackb(result_bytes, raw=False, strict_map_key=False)

                result = await asyncio.to_thread(read_response)

                bt.logging.info(f"Read reporting response data ({time.time()-read_start:.4f}s)")
                self.initial_balances_published = result['initial_balances_published']
                self.miner_stats = result['miner_stats']

            except Exception as e:
                bt.logging.error(f"Error sending to reporting service: {e}")
                import traceback
                bt.logging.error(traceback.format_exc())
            finally:
                self._reporting = False
                bt.logging.info(f"Completed reporting for step {reporting_step} ({time.time() - start}s)")

        def report(self) -> None:
            if self.config.reporting.disabled or not self.last_state or self.last_state.timestamp % self.config.scoring.interval != 0:
                return
            if self._reporting:
                bt.logging.warning(f"Skipping reporting at step {self.step} — previous report still running.")
                return
            bt.logging.debug(f"[REPORT] Scheduling from thread: {threading.current_thread().name}")
            self.main_loop.call_soon_threadsafe(lambda: self.main_loop.create_task(self._report()))

        async def _compute_compact_volumes(self) -> Dict:
            """
            Compute compact volume metrics used for activity-weighted scoring.

            This method aggregates each UID’s total trade volumes across all books
            into two compressed metrics:

            • **lookback_volume** — Total traded value within the scoring lookback window
            • **latest_volume** — Most recent sampled trade volume entry

            These values are consumed later during reward computation for activity scoring.

            Returns:
                Dict[int, Dict[int, Dict[str, float]]]:
                    Nested structure:
                    {
                        uid: {
                            book_id: {
                                "lookback_volume": float,
                                "latest_volume": float
                            }
                        }
                    }
            """
            lookback_threshold = self.simulation_timestamp - (
                self.config.scoring.sharpe.lookback *
                self.simulation.publish_interval
            )

            compact_volumes = {}
            for uid in self.metagraph.uids:
                uid_item = uid.item()
                compact_volumes[uid_item] = {}

                if uid_item in self.trade_volumes:
                    for book_id, book_volume in self.trade_volumes[uid_item].items():
                        total_trades = book_volume['total']
                        if not total_trades:
                            compact_volumes[uid_item][book_id] = {
                                'lookback_volume': 0.0,
                                'latest_volume': 0.0
                            }
                            continue
                        timestamps = total_trades.keys()
                        latest_time = max(timestamps)
                        latest_volume = total_trades[latest_time]
                        lookback_volume = sum(
                            vol for t, vol in total_trades.items()
                            if t >= lookback_threshold
                        )
                        compact_volumes[uid_item][book_id] = {
                            'lookback_volume': lookback_volume,
                            'latest_volume': latest_volume
                        }
                else:
                    for book_id in range(self.simulation.book_count):
                        compact_volumes[uid_item][book_id] = {
                            'lookback_volume': 0.0,
                            'latest_volume': 0.0
                        }
            return compact_volumes

        async def _compute_compact_roundtrip_volumes(self) -> Dict:
            """
            Compute compact round-trip volume metrics for realized Sharpe activity scoring.
            Round-trip volume represents trades that opened AND closed positions,
            indicating actual realized trading activity rather than just position building.

            Returns:
                Dict[int, Dict[int, Dict[str, float]]]:
                    {
                        uid: {
                            book_id: {
                                "lookback_roundtrip_volume": float,
                                "latest_roundtrip_volume": float
                            }
                        }
                    }
            """
            lookback_threshold = self.simulation_timestamp - (
                self.config.scoring.sharpe.lookback *
                self.simulation.publish_interval
            )

            compact_roundtrip = {}
            for uid in self.metagraph.uids:
                uid_item = uid.item()
                compact_roundtrip[uid_item] = {}

                if uid_item in self.roundtrip_volumes:
                    for book_id, rt_volumes in self.roundtrip_volumes[uid_item].items():
                        if not rt_volumes:
                            compact_roundtrip[uid_item][book_id] = {
                                'lookback_roundtrip_volume': 0.0,
                                'latest_roundtrip_volume': 0.0
                            }
                            continue

                        timestamps = rt_volumes.keys()
                        latest_time = max(timestamps)
                        latest_volume = rt_volumes[latest_time]

                        lookback_volume = sum(
                            vol for t, vol in rt_volumes.items()
                            if t >= lookback_threshold
                        )

                        compact_roundtrip[uid_item][book_id] = {
                            'lookback_roundtrip_volume': lookback_volume,
                            'latest_roundtrip_volume': latest_volume
                        }
                else:
                    for book_id in range(self.simulation.book_count):
                        compact_roundtrip[uid_item][book_id] = {
                            'lookback_roundtrip_volume': 0.0,
                            'latest_roundtrip_volume': 0.0
                        }

            return compact_roundtrip

        def _match_trade_fifo(self, uid: int, book_id: int, is_buy: bool, quantity: float,
                            price: float, fee: float, timestamp: int) -> tuple[float, float]:
            """
            FIFO matching including fee accounting.
            Args:
                uid: Miner UID
                book_id: Book identifier
                is_buy: True if buying (going long), False if selling (going short)
                quantity: Trade quantity
                price: Trade price
                fee: Fee paid for this trade (positive = cost, negative = rebate)
                timestamp: Trade timestamp

            Returns:
                tuple[float, float]: (realized_pnl, roundtrip_volume)
                    - realized_pnl: Realized P&L from matched trades (including fees)
                    - roundtrip_volume: Total quantity that completed a round-trip
            """
            positions = self.open_positions[uid][book_id]

            if is_buy:
                shorts = positions['shorts']
                if not shorts:
                    positions['longs'].append((timestamp, quantity, price, fee))
                    return 0.0, 0.0
            else:
                longs = positions['longs']
                if not longs:
                    positions['shorts'].append((timestamp, quantity, price, fee))
                    return 0.0, 0.0

            realized_pnl = 0.0
            roundtrip_volume = 0.0
            remaining_qty = quantity

            quantity_inv = 1.0 / quantity if quantity > 0 else 0.0

            if is_buy:
                # Buying: close shorts first (FIFO), then open longs
                while remaining_qty > 0 and shorts:
                    old_ts, old_qty, old_price, old_fee = shorts[0]

                    if old_qty <= remaining_qty:
                        # Fully close this short position
                        price_pnl = (old_price - price) * old_qty
                        close_fee = fee * old_qty * quantity_inv
                        realized_pnl += price_pnl - old_fee - close_fee
                        roundtrip_volume += old_qty
                        remaining_qty -= old_qty
                        shorts.popleft()
                    else:
                        # Partially close short position
                        old_qty_inv = 1.0 / old_qty

                        price_pnl = (old_price - price) * remaining_qty
                        close_fee = fee  # Entire trade closes positions
                        open_fee = old_fee * remaining_qty * old_qty_inv
                        realized_pnl += price_pnl - open_fee - close_fee
                        roundtrip_volume += remaining_qty

                        # Update remaining position with reduced fee
                        remaining_position_fee = old_fee - open_fee
                        shorts[0] = (old_ts, old_qty - remaining_qty, old_price, remaining_position_fee)
                        remaining_qty = 0

                # Any remaining quantity opens new long position
                if remaining_qty > 0:
                    open_fee = fee * remaining_qty * quantity_inv
                    positions['longs'].append((timestamp, remaining_qty, price, open_fee))

            else:
                # Selling: close longs first (FIFO), then open shorts
                while remaining_qty > 0 and longs:
                    old_ts, old_qty, old_price, old_fee = longs[0]

                    if old_qty <= remaining_qty:
                        # Fully close this long position
                        price_pnl = (price - old_price) * old_qty
                        close_fee = fee * old_qty * quantity_inv
                        realized_pnl += price_pnl - old_fee - close_fee
                        roundtrip_volume += old_qty
                        remaining_qty -= old_qty
                        longs.popleft()
                    else:
                        # Partially close long position
                        old_qty_inv = 1.0 / old_qty

                        price_pnl = (price - old_price) * remaining_qty
                        close_fee = fee  # Entire trade closes positions
                        open_fee = old_fee * remaining_qty * old_qty_inv
                        realized_pnl += price_pnl - open_fee - close_fee
                        roundtrip_volume += remaining_qty

                        # Update remaining position with reduced fee
                        remaining_position_fee = old_fee - open_fee
                        longs[0] = (old_ts, old_qty - remaining_qty, old_price, remaining_position_fee)
                        remaining_qty = 0

                # Any remaining quantity opens new short position
                if remaining_qty > 0:
                    open_fee = fee * remaining_qty * quantity_inv
                    positions['shorts'].append((timestamp, remaining_qty, price, open_fee))

            return realized_pnl, roundtrip_volume

        async def _update_trade_volumes(self, state: MarketSimulationStateUpdate):
            """
            Updates and maintains all trade volume tracking and position accounting structures.

            This function processes raw trade events from the simulator state and updates
            the following per-UID per-book time series:

            **Volume Tracking:**
            • **total** — total traded notional value
            • **maker** — maker-side volume
            • **taker** — taker-side volume
            • **self** — trades where maker == taker
            • **roundtrip_volumes** — volume from completed round-trip trades (open + close)
            • **volume_sums** / **maker_volume_sums** / **taker_volume_sums** / **self_volume_sums** / **roundtrip_volume_sums**

            **Position Accounting (FIFO):**
            • **open_positions** — tracks open long/short positions with (timestamp, quantity, price, fee)
            • **realized_pnl_history** — realized profit/loss from closed positions (fee-adjusted)
            • Matches trades via FIFO to calculate realized P&L and round-trip volume

            **Inventory & History:**
            • **inventory_history** — mark-to-market inventory value changes over time
            • **recent_trades** — rolling buffer of last 25 trades per book
            • **recent_miner_trades** — rolling buffer of last 5 trades per miner per book
            • **initial_balances** — baseline balances for inventory value calculations

            **Operations:**
            • Samples volume at aligned timestamps (trade_volume_sampling_interval)
            • Prunes old volume entries outside assessment window (trade_volume_assessment_period)
            • Prunes old inventory and realized P&L history outside Sharpe lookback window
            • Batch processes updates for performance (deferred rounding)
            • Ensures all nested structures are initialized dynamically

            Args:
                state (MarketSimulationStateUpdate):
                    Full simulation tick state containing books, accounts, and notices.

            Returns:
                None

            Raises:
                Logs errors when UID-level processing fails but continues processing remaining UIDs.
            """
            total_start = time.time()

            books = state.books
            timestamp = state.timestamp
            accounts = state.accounts
            notices = state.notices

            volume_decimals = self.simulation.volumeDecimals
            book_count = self.simulation.book_count

            sampled_timestamp = math.ceil(
                timestamp / self.config.scoring.activity.trade_volume_sampling_interval
            ) * self.config.scoring.activity.trade_volume_sampling_interval

            volume_prune_threshold = timestamp - self.config.scoring.activity.trade_volume_assessment_period

            for bookId, book in books.items():
                trades = [event for event in book['e'] if event['y'] == 't']
                if trades:
                    recent_trades_book = self.recent_trades[bookId]
                    recent_trades_book.extend([TradeInfo.model_construct(**t) for t in trades])
                    del recent_trades_book[:-25]

            volume_deltas = defaultdict(lambda: defaultdict(lambda: {'total': 0.0, 'maker': 0.0, 'taker': 0.0, 'self': 0.0}))
            realized_pnl_updates = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
            roundtrip_volume_updates = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
            uids_to_round = set()

            for uid in self.metagraph.uids:
                uid_item = uid.item()
                try:
                    # Initialize trade volumes structure if needed
                    if uid_item not in self.trade_volumes:
                        self.trade_volumes[uid_item] = {
                            book_id: {'total': {}, 'maker': {}, 'taker': {}, 'self': {}}
                            for book_id in range(book_count)
                        }
                    trade_volumes_uid = self.trade_volumes[uid_item]

                    # Prune old volumes and update sums
                    for book_id, role_trades in trade_volumes_uid.items():
                        for role, trades in role_trades.items():
                            if trades:
                                pruned_volume = sum(
                                    v for t, v in trades.items() if t < volume_prune_threshold
                                )
                                if pruned_volume > 0:
                                    # Update sums without immediate rounding
                                    if role == 'total':
                                        self.volume_sums[uid_item][book_id] = max(
                                            0.0, self.volume_sums[uid_item][book_id] - pruned_volume
                                        )
                                    elif role == 'maker':
                                        self.maker_volume_sums[uid_item][book_id] = max(
                                            0.0, self.maker_volume_sums[uid_item][book_id] - pruned_volume
                                        )
                                    elif role == 'taker':
                                        self.taker_volume_sums[uid_item][book_id] = max(
                                            0.0, self.taker_volume_sums[uid_item][book_id] - pruned_volume
                                        )
                                    elif role == 'self':
                                        self.self_volume_sums[uid_item][book_id] = max(
                                            0.0, self.self_volume_sums[uid_item][book_id] - pruned_volume
                                        )
                                    uids_to_round.add(uid_item)

                                trade_volumes_uid[book_id][role] = {
                                    t: v for t, v in trades.items() if t >= volume_prune_threshold
                                }

                    # Initialize sampled timestamp entries
                    for book_id in range(book_count):
                        if book_id not in trade_volumes_uid:
                            trade_volumes_uid[book_id] = {
                                'total': {}, 'maker': {}, 'taker': {}, 'self': {}
                            }
                        book_trade_volumes = trade_volumes_uid[book_id]
                        if sampled_timestamp not in book_trade_volumes['total']:
                            book_trade_volumes['total'][sampled_timestamp] = 0.0
                            book_trade_volumes['maker'][sampled_timestamp] = 0.0
                            book_trade_volumes['taker'][sampled_timestamp] = 0.0
                            book_trade_volumes['self'][sampled_timestamp] = 0.0

                    # Process trade notices
                    if uid_item in notices:
                        trades = [notice for notice in notices[uid_item] if notice['y'] in ['EVENT_TRADE', "ET"]]
                        if trades:
                            recent_miner_trades_uid = self.recent_miner_trades[uid_item]

                            for trade in trades:
                                is_maker = trade['Ma'] == uid_item
                                is_taker = trade['Ta'] == uid_item
                                book_id = trade['b']

                                # Update recent miner trades
                                if is_maker:
                                    recent_miner_trades_uid[book_id].append([TradeEvent.model_construct(**trade), "maker"])
                                if is_taker:
                                    recent_miner_trades_uid[book_id].append([TradeEvent.model_construct(**trade), "taker"])
                                recent_miner_trades_uid[book_id] = recent_miner_trades_uid[book_id][-5:]

                                book_volumes = trade_volumes_uid[book_id]
                                trade_value = trade['q'] * trade['p']

                                book_volumes['total'][sampled_timestamp] += trade_value
                                volume_deltas[uid_item][book_id]['total'] += trade_value

                                if trade['Ma'] == trade['Ta']:
                                    book_volumes['self'][sampled_timestamp] += trade_value
                                    volume_deltas[uid_item][book_id]['self'] += trade_value
                                elif is_maker:
                                    book_volumes['maker'][sampled_timestamp] += trade_value
                                    volume_deltas[uid_item][book_id]['maker'] += trade_value
                                elif is_taker:
                                    book_volumes['taker'][sampled_timestamp] += trade_value
                                    volume_deltas[uid_item][book_id]['taker'] += trade_value

                                uids_to_round.add(uid_item)

                                # FIFO Matching: Calculate realized P&L and round-trip volume
                                quantity = trade['q']
                                price = trade['p']
                                side = trade['s']

                                is_buy = (is_taker and side == 0) or (is_maker and side == 1)
                                fee = trade['Mf'] if is_maker else trade['Tf']

                                realized_pnl, roundtrip_volume = self._match_trade_fifo(
                                    uid_item, book_id, is_buy, quantity, price, fee, timestamp
                                )

                                if realized_pnl != 0.0:
                                    realized_pnl_updates[uid_item][timestamp][book_id] += realized_pnl

                                if roundtrip_volume > 0:
                                    roundtrip_value = roundtrip_volume * price
                                    roundtrip_volume_updates[uid_item][sampled_timestamp][book_id] += roundtrip_value

                            for book_id, deltas in volume_deltas[uid_item].items():
                                self.volume_sums[uid_item][book_id] += deltas['total']
                                self.maker_volume_sums[uid_item][book_id] += deltas['maker']
                                self.taker_volume_sums[uid_item][book_id] += deltas['taker']
                                self.self_volume_sums[uid_item][book_id] += deltas['self']

                    # Initialize zero P&L for timestamps with no trades
                    if timestamp not in self.realized_pnl_history[uid_item]:
                        self.realized_pnl_history[uid_item][timestamp] = {
                            book_id: 0.0 for book_id in range(book_count)
                        }

                    # Update inventory history
                    if uid_item in accounts:
                        initial_balances_uid = self.initial_balances[uid_item]
                        accounts_uid = accounts[uid_item]

                        for bookId, account in accounts_uid.items():
                            initial_balance_book = initial_balances_uid[bookId]
                            if initial_balance_book['BASE'] is None:
                                initial_balance_book['BASE'] = account['bb']['t']
                            if initial_balance_book['QUOTE'] is None:
                                initial_balance_book['QUOTE'] = account['qb']['t']
                            if initial_balance_book['WEALTH'] is None:
                                initial_balance_book['WEALTH'] = get_inventory_value(account, books[bookId])

                        self.inventory_history[uid_item][timestamp] = {
                            book_id: get_inventory_value(accounts_uid[book_id], book) - initial_balances_uid[book_id]['WEALTH']
                            for book_id, book in books.items()
                        }
                    else:
                        self.inventory_history[uid_item][timestamp] = {book_id: 0.0 for book_id in books}

                    inventory_hist = self.inventory_history[uid_item]
                    if len(inventory_hist) > self.config.scoring.sharpe.lookback:
                        timestamps_to_keep = sorted(inventory_hist.keys())[-self.config.scoring.sharpe.lookback:]
                        self.inventory_history[uid_item] = {
                            ts: inventory_hist[ts] for ts in timestamps_to_keep
                        }

                    pnl_hist = self.realized_pnl_history[uid_item]
                    if len(pnl_hist) > self.config.scoring.sharpe.lookback:
                        timestamps_to_keep = sorted(pnl_hist.keys())[-self.config.scoring.sharpe.lookback:]
                        self.realized_pnl_history[uid_item] = {
                            ts: pnl_hist[ts] for ts in timestamps_to_keep
                        }

                    if uid_item in self.roundtrip_volumes:
                        roundtrip_volumes_uid = self.roundtrip_volumes[uid_item]
                        for book_id, rt_volumes in roundtrip_volumes_uid.items():
                            if rt_volumes:
                                pruned_rt_volume = sum(
                                    v for t, v in rt_volumes.items() if t < volume_prune_threshold
                                )
                                if pruned_rt_volume > 0:
                                    current = self.roundtrip_volume_sums[uid_item][book_id]
                                    self.roundtrip_volume_sums[uid_item][book_id] = max(0.0, current - pruned_rt_volume)
                                    uids_to_round.add(uid_item)

                                roundtrip_volumes_uid[book_id] = {
                                    t: v for t, v in rt_volumes.items() if t >= volume_prune_threshold
                                }

                except Exception as ex:
                    bt.logging.error(f"Failed to update trade data for UID {uid_item}: {ex}")
                    bt.logging.error(traceback.format_exc())

            for uid_item, timestamps in realized_pnl_updates.items():
                for ts, books in timestamps.items():
                    for book_id, pnl in books.items():
                        self.realized_pnl_history[uid_item][ts][book_id] += pnl

            for uid_item, timestamps in roundtrip_volume_updates.items():
                for ts, books in timestamps.items():
                    for book_id, rt_vol in books.items():
                        _ = self.roundtrip_volumes[uid_item][book_id]
                        if uid_item not in self.roundtrip_volumes:
                            self.roundtrip_volumes[uid_item] = defaultdict(lambda: defaultdict(float))
                        if book_id not in self.roundtrip_volumes[uid_item]:
                            self.roundtrip_volumes[uid_item][book_id] = defaultdict(float)
                        if ts not in self.roundtrip_volumes[uid_item][book_id]:
                            self.roundtrip_volumes[uid_item][book_id][ts] = 0.0
                        self.roundtrip_volumes[uid_item][book_id][ts] += rt_vol
                        self.roundtrip_volume_sums[uid_item][book_id] += rt_vol
                        uids_to_round.add(uid_item)

            for uid_item in uids_to_round:
                for book_id in range(book_count):
                    if uid_item in self.trade_volumes and book_id in self.trade_volumes[uid_item]:
                        book_vols = self.trade_volumes[uid_item][book_id]
                        for role in ['total', 'maker', 'taker', 'self']:
                            if sampled_timestamp in book_vols[role]:
                                book_vols[role][sampled_timestamp] = round(
                                    book_vols[role][sampled_timestamp],
                                    volume_decimals
                                )
                    if uid_item in self.volume_sums and book_id in self.volume_sums[uid_item]:
                        self.volume_sums[uid_item][book_id] = round(
                            self.volume_sums[uid_item][book_id],
                            volume_decimals
                        )
                    if uid_item in self.maker_volume_sums and book_id in self.maker_volume_sums[uid_item]:
                        self.maker_volume_sums[uid_item][book_id] = round(
                            self.maker_volume_sums[uid_item][book_id],
                            volume_decimals
                        )
                    if uid_item in self.taker_volume_sums and book_id in self.taker_volume_sums[uid_item]:
                        self.taker_volume_sums[uid_item][book_id] = round(
                            self.taker_volume_sums[uid_item][book_id],
                            volume_decimals
                        )
                    if uid_item in self.self_volume_sums and book_id in self.self_volume_sums[uid_item]:
                        self.self_volume_sums[uid_item][book_id] = round(
                            self.self_volume_sums[uid_item][book_id],
                            volume_decimals
                        )
                    if uid_item in self.roundtrip_volume_sums and book_id in self.roundtrip_volume_sums[uid_item]:
                        self.roundtrip_volume_sums[uid_item][book_id] = round(
                            self.roundtrip_volume_sums[uid_item][book_id],
                            volume_decimals
                        )
                if uid_item in realized_pnl_updates:
                    for ts in realized_pnl_updates[uid_item]:
                        for book_id in range(book_count):
                            if ts in self.realized_pnl_history[uid_item]:
                                self.realized_pnl_history[uid_item][ts][book_id] = round(
                                    self.realized_pnl_history[uid_item][ts][book_id],
                                    volume_decimals
                                )

            total_time = time.time() - total_start
            bt.logging.debug(f"[UPDATE_VOLUMES] Total: {total_time:.4f}s")
            await asyncio.sleep(0)

        async def _reward(self, state : MarketSimulationStateUpdate):
            """
            Asynchronously perform the full reward computation pipeline.

            This function is executed within an async lock to ensure that reward
            calculation never overlaps across threads or scheduler ticks.

            Steps Performed:
                1. Acquire async lock to prevent concurrent reward computation.
                2. Update trade volumes via `_update_trade_volumes()`.
                3. If the timestamp does not align with the scoring interval, exit early.
                4. Convert inventory history into compact, lookback-bounded form.
                5. Compute compact volume metrics.
                6. Extract current miner balances from simulation state.
                7. Construct the complete `validator_data` payload for the scoring engine.
                8. Call the reward function (`get_rewards`) to compute:
                    • Sharpe values (both unrealized and realized)
                    • Activity factors
                    • Updated simulation timestamp
                    • Final per-UID reward values
                9. Apply computed rewards and update internal score tables.

            Args:
                state (MarketSimulationStateUpdate):
                    Full simulation tick state used as the basis for scoring.

            Logs:
                • Execution times for major phases
                • Reward calculation progress
                • Detailed traceback in case of failure
            """
            if not hasattr(self, "_reward_lock"):
                self._reward_lock = asyncio.Lock()

            start_wait = time.time()
            rewarding_step = self.step
            async with self._reward_lock:
                waited = time.time() - start_wait
                if waited > 0:
                    bt.logging.debug(f"Acquired reward lock after waiting {waited:.3f}s")
                await asyncio.sleep(0)

                self.shared_state_rewarding = True
                await asyncio.sleep(0)

                timestamp = state.timestamp
                duration = duration_from_timestamp(timestamp)
                bt.logging.info(f"Starting reward calculation for step {rewarding_step}...")
                start = time.time()
                await asyncio.sleep(0)

                try:
                    bt.logging.debug("[REWARD] Updating trade volumes...")
                    update_start = time.time()
                    await self._update_trade_volumes(state)
                    bt.logging.debug(f"[REWARD] Trade volumes updated in {time.time()-update_start:.4f}s")
                    if timestamp % self.config.scoring.interval != 0:
                        bt.logging.info(f"Agent Scores Data Updated for {duration} ({time.time()-start:.4f}s)")
                        return
                    bt.logging.debug("[REWARD] Converting inventory history...")
                    convert_start = time.time()
                    inventory_compact = {}

                    total_timestamps = 0
                    for uid in self.metagraph.uids:
                        uid_item = uid.item()
                        if uid_item in self.inventory_history and len(self.inventory_history[uid_item]) > 0:
                            hist = self.inventory_history[uid_item]
                            lookback = min(self.config.scoring.sharpe.lookback, len(hist))
                            sorted_timestamps = sorted(hist.keys())[-lookback:]
                            inventory_compact[uid_item] = {ts: hist[ts] for ts in sorted_timestamps}
                            total_timestamps += len(sorted_timestamps)
                        else:
                            inventory_compact[uid_item] = {}

                    bt.logging.debug(f"[REWARD] Converted inventory history in {time.time()-convert_start:.4f}s")

                    compact_start = time.time()
                    compact_volumes = await self._compute_compact_volumes()
                    bt.logging.debug(f"[REWARD] Computed compact volumes in {time.time()-compact_start:.4f}s")

                    roundtrip_start = time.time()
                    compact_roundtrip_volumes = await self._compute_compact_roundtrip_volumes()
                    bt.logging.debug(f"[REWARD] Computed compact round-trip volumes in {time.time()-roundtrip_start:.4f}s")

                    bt.logging.debug("[REWARD] Converting realized P&L history...")
                    convert_start = time.time()

                    realized_pnl_compact = {}
                    for uid in self.metagraph.uids:
                        uid_item = uid.item()
                        if uid_item in self.realized_pnl_history and len(self.realized_pnl_history[uid_item]) > 0:
                            hist = self.realized_pnl_history[uid_item]
                            lookback = min(self.config.scoring.sharpe.lookback, len(hist))
                            sorted_timestamps = sorted(hist.keys())[-lookback:]
                            realized_pnl_compact[uid_item] = {ts: hist[ts] for ts in sorted_timestamps}
                        else:
                            realized_pnl_compact[uid_item] = {}

                    bt.logging.debug(f"[REWARD] Converted realized P&L history in {time.time()-convert_start:.4f}s")

                    bt.logging.debug("[REWARD] Extracting miner positions...")
                    positions_start = time.time()
                    miner_positions = {}

                    for uid in self.metagraph.uids:
                        uid_item = uid.item()
                        if uid_item in state.accounts:
                            miner_positions[uid_item] = {}
                            for book_id, account in state.accounts[uid_item].items():
                                miner_positions[uid_item][book_id] = {
                                    'base': account['bb']['t'] - account['bl'] + account['bc'],
                                    'quote': account['qb']['t'] - account['ql'] + account['qc'],
                                    'midquote': round((state.books[book_id]['b'][0]['p'] + state.books[book_id]['a'][0]['p']) / 2, self.simulation.priceDecimals)
                                }

                    bt.logging.debug(f"[REWARD] Extracted positions for {len(miner_positions)} miners in {time.time()-positions_start:.4f}s")

                    prep_start = time.time()
                    validator_data = {
                        'sharpe_values': self.sharpe_values,
                        'activity_factors': self.activity_factors,
                        'activity_factors_realized': self.activity_factors_realized,
                        'compact_volumes': compact_volumes,
                        'compact_roundtrip_volumes': compact_roundtrip_volumes,
                        'inventory_history': inventory_compact,
                        'realized_pnl_history': realized_pnl_compact,
                        'miner_positions': miner_positions,
                        'config': {
                            'scoring': {
                                'sharpe': {
                                    'normalization_min': self.config.scoring.sharpe.normalization_min,
                                    'normalization_max': self.config.scoring.sharpe.normalization_max,
                                    'lookback': self.config.scoring.sharpe.lookback,
                                    'min_lookback': self.config.scoring.sharpe.min_lookback,
                                    'min_realized_observations': self.config.scoring.sharpe.min_realized_observations,
                                    'parallel_workers': self.config.scoring.sharpe.parallel_workers if self.config.scoring.sharpe.parallel_workers > 0 else multiprocessing.cpu_count() // 2,
                                },
                                'activity': {
                                    'capital_turnover_cap': self.config.scoring.activity.capital_turnover_cap,
                                    'trade_volume_sampling_interval': self.config.scoring.activity.trade_volume_sampling_interval,
                                    'trade_volume_assessment_period': self.config.scoring.activity.trade_volume_assessment_period,
                                },
                                "inventory" : {
                                    'min_balance_ratio_multiplier': self.config.scoring.inventory.min_balance_ratio_multiplier,
                                    'max_balance_ratio_multiplier': self.config.scoring.inventory.max_balance_ratio_multiplier
                                },
                                'interval': self.config.scoring.interval,
                            },
                            'rewarding': {
                                'seed': self.config.rewarding.seed,
                                'pareto': {
                                    'shape': self.config.rewarding.pareto.shape,
                                    'scale': self.config.rewarding.pareto.scale,
                                }
                            },
                        },
                        'simulation_config': {
                            'miner_wealth': self.simulation.miner_wealth,
                            'publish_interval': self.simulation.publish_interval,
                            'volumeDecimals': self.simulation.volumeDecimals,
                            'grace_period': self.simulation.grace_period,
                        },
                        'reward_weights': self.reward_weights,
                        'simulation_timestamp': self.simulation_timestamp,
                        'uids': [uid.item() for uid in self.metagraph.uids],
                        'deregistered_uids': self.deregistered_uids,
                        'device': self.device,
                    }
                    prep_time = time.time() - prep_start
                    bt.logging.debug(f"[REWARD] Prepared validator_data in {prep_time:.4f}s")

                    await asyncio.sleep(0)

                    rewards, updated_data = get_rewards(validator_data)

                    self.sharpe_values = updated_data.get('sharpe_values', self.sharpe_values)
                    self.activity_factors = updated_data.get('activity_factors', self.activity_factors)
                    self.activity_factors_realized = updated_data.get('activity_factors_realized', self.activity_factors_realized)
                    self.simulation_timestamp = updated_data.get('simulation_timestamp', self.simulation_timestamp)

                    bt.logging.debug(f"Agent Rewards Recalculated for {duration} ({time.time()-start:.4f}s):\n{rewards}")
                    self.update_scores(rewards, self.metagraph.uids)
                    bt.logging.info(f"Agent Scores Updated for {duration} ({time.time()-start:.4f}s)")

                except Exception as ex:
                    self.pagerduty_alert(f"Rewarding failed: {ex}", details={"trace": traceback.format_exc()})
                finally:
                    self.shared_state_rewarding = False
                    await asyncio.sleep(0)
                    bt.logging.debug(f"Completed rewarding (TOTAL {time.time()-start_wait:.4f}s).")
            await asyncio.sleep(0)

        def reward(self, state : MarketSimulationStateUpdate) -> None:
            """
            Schedule asynchronous reward calculation on the validator's main event loop.
            Offloads work to `_reward()` to ensure that:

            • Reward computation always occurs in the correct asyncio event loop
            • CPU-intensive work does not block the calling thread
            • Reward logic executes with proper async locking

            Args:
                state (MarketSimulationStateUpdate):
                    Simulation state for the current tick, forwarded to `_reward`.
            """
            bt.logging.debug(f"[REWARD] Scheduling from thread: {threading.current_thread().name}")
            bt.logging.debug(f"[REWARD] Main loop ID: {id(self.main_loop)}, Current loop ID: {id(asyncio.get_event_loop())}")
            self.main_loop.call_soon_threadsafe(lambda: self.main_loop.create_task(self._reward(state)))

        async def handle_state(self, message : dict, state : MarketSimulationStateUpdate, receive_start : int) -> dict:
            """
            Handle a full simulator state update, enrich it with validator data, compute responses,
            update internal validator state, and return instructions back to the simulator.


            This method is the central processing loop for each simulation step. It performs:
            - Periodic validator configuration reloads.
            - Per‑account volume injection.
            - Simulation metadata updates and logging.
            - State forwarding to miners and response aggregation.
            - Reward calculation, scoring, persistence, and metric publication.


            Args:
            message (dict): The raw simulator state message as received (typically msgpack‑decoded).
            state (MarketSimulationStateUpdate): Parsed simulation state model containing orderbooks, accounts, timestamps, etc.
            receive_start (int): Timestamp marking when the simulator delivered the message, used for latency metrics.


            Returns:
            dict: Serialized response batch to be returned to the simulator.
            """
            # Every 1H of simulation time, check if there are any changes to the validator - if updates exist, pull them and restart.
            if self.simulation_timestamp % 3600_000_000_000 == 0 and self.simulation_timestamp != 0:
                bt.logging.info("Checking for validator updates...")
                self.update_repo()
            state.version = __spec_version__
            start = time.time()
            for uid, accounts in state.accounts.items():
                for book_id in accounts:
                    state.accounts[uid][book_id]['v'] = self.volume_sums.get((uid, book_id), 0.0)
            bt.logging.info(f"Volumes added to state ({time.time()-start:.4f}s).")

            # Update variables
            if not self.start_time:
                self.start_time = time.time()
                self.start_timestamp = state.timestamp
            if self.simulation.logDir != message['logDir']:
                bt.logging.info(f"Simulation log directory changed : {self.simulation.logDir} -> {message['logDir']}")
                self.simulation.logDir = message['logDir']
            self.simulation_timestamp = state.timestamp
            self.step_rates.append((state.timestamp - (self.last_state.timestamp if self.last_state else self.start_timestamp)) / (time.time() - (self.last_state_time if self.last_state_time else self.start_time)))
            self.last_state = state
            if self.simulation:
                self.simulation.simulation_id = os.path.basename(self.simulation.logDir)[:13]
                state.config = self.simulation.model_copy()
                state.config.logDir = None
            self.step += 1

            if self.simulation_timestamp % self.simulation.log_window == self.simulation.publish_interval:
                self.compress_outputs()

            # Log received state data
            bt.logging.info(f"STATE UPDATE RECEIVED | VALIDATOR STEP : {self.step} | TIME : {duration_from_timestamp(state.timestamp)} (T={state.timestamp})")
            if self.config.logging.debug or self.config.logging.trace:
                debug_text = ''
                for bookId, book in state.books.items():
                    debug_text += '-' * 50 + "\n"
                    debug_text += f"BOOK {bookId}" + "\n"
                    if book['b'] and book['a']:
                        debug_text += ' | '.join([f"{level['q']:.4f}@{level['p']}" for level in reversed(book['b'][:5])]) + '||' + ' | '.join([f"{level['q']:.4f}@{level['p']}" for level in book['a'][:5]]) + "\n"
                    else:
                        debug_text += "EMPTY" + "\n"
                bt.logging.debug("\n" + debug_text.strip("\n"))

            # Process deregistration notices
            self.process_resets(state)
            # Forward state synapse to miners, populate response data to simulator object and serialize for returning to simulator.
            start = time.time()
            response = SimulatorResponseBatch(await forward(self, state))
            bt.logging.debug(f"Gathered Response Batch ({time.time()-start}s)")
            start = time.time()
            response = response.serialize()
            bt.logging.debug(f"Serialized Response Batch ({time.time()-start}s)")
            # Log response data, start state serialization and reporting threads, and return miner instructions to the simulator
            if len(response['responses']) > 0:
                bt.logging.trace(f"RESPONSE : {response}")
            bt.logging.info(f"RATE : {(self.step_rates[-1] if self.step_rates != [] else 0) / 1e9:.2f} STEPS/s | AVG : {(sum(self.step_rates) / len(self.step_rates) / 1e9 if self.step_rates != [] else 0):.2f}  STEPS/s")
            self.step_rates = self.step_rates[-10000:]
            self.last_state_time = time.time()

            # Calculate latest rewards, update miner scores, save state and publish metrics
            self.maintain()
            self.reward(state)
            self.save_state()
            self.report()
            bt.logging.info(f"State update handled ({time.time()-receive_start}s)")

            return response

        async def _listen(self):
            """
            Continuously listen for simulator state updates via POSIX IPC, unpack them,
            parse them into state objects, and process them with `handle_state`.

            This listener runs the full validator event loop when operating in IPC mode.
            It performs:
            - Receiving shared‑memory pointers via message queues.
            - mmap reads with retry logic.
            - msgpack unpacking with retry logic.
            - Parsing the state dict into a typed model.
            - Handling all state updates and forwarding miner responses.

            The method uses run‑in‑executor offloading for blocking IPC operations.
            """
            def receive(mq_req: posix_ipc.MessageQueue) -> tuple:
                msg, priority = mq_req.receive()
                receive_start = time.time()
                bt.logging.info(f"Received state update from simulator (msgpack)")
                byte_size_req = int.from_bytes(msg, byteorder="little")
                shm_req = posix_ipc.SharedMemory("/state")
                start = time.time()
                packed_data = None
                for attempt in range(1, 6):
                    try:
                        with mmap.mmap(shm_req.fd, byte_size_req, mmap.MAP_SHARED, mmap.PROT_READ) as mm:
                            packed_data = mm.read(byte_size_req)
                        break
                    except Exception as ex:
                        if attempt < 5:
                            bt.logging.error(f"mmap read failed (attempt {attempt}/5): {ex}")
                            time.sleep(0.005)
                        else:
                            bt.logging.error(f"mmap read failed on all 5 attempts: {ex}")
                            return None, receive_start
                    finally:
                        if packed_data is not None or attempt >= 5:
                            shm_req.close_fd()
                bt.logging.info(f"Retrieved State Update ({time.time() - receive_start}s)")
                start = time.time()
                for attempt in range(1, 6):
                    try:
                        result = msgpack.unpackb(packed_data, raw=False, use_list=True, strict_map_key=False)
                        bt.logging.info(f"Unpacked state update ({time.time() - start:.4f}s)")
                        break
                    except Exception as ex:
                        if attempt < 5:
                            bt.logging.error(f"Msgpack unpack failed (attempt {attempt}/5): {ex}")
                            time.sleep(0.005)
                        else:
                            bt.logging.error(f"Msgpack unpack failed on all 5 attempts: {ex}")
                            return None, receive_start
                return result, receive_start

            def respond(response: dict) -> dict:
                self.last_response = response
                packed_res = msgpack.packb(response, use_bin_type=True)
                byte_size_res = len(packed_res)
                mq_res = posix_ipc.MessageQueue("/taosim-res", flags=posix_ipc.O_CREAT, max_messages=1, max_message_size=8)
                shm_res = posix_ipc.SharedMemory("/responses", flags=posix_ipc.O_CREAT, size=byte_size_res)
                with mmap.mmap(shm_res.fd, byte_size_res, mmap.MAP_SHARED, mmap.PROT_WRITE | mmap.PROT_READ) as mm:
                    shm_res.close_fd()
                    mm.write(packed_res)
                mq_res.send(byte_size_res.to_bytes(8, byteorder="little"))
                mq_res.close()

            mq_req = posix_ipc.MessageQueue("/taosim-req", flags=posix_ipc.O_CREAT, max_messages=1, max_message_size=8)
            thread_pool = ThreadPoolExecutor(max_workers=4)
            try:
                while True:
                    response = {"responses": []}
                    try:
                        loop = asyncio.get_event_loop()
                        t1 = time.time()
                        bt.logging.debug(f"[LISTEN] Starting receive at {t1:.3f}")
                        message, receive_start = await loop.run_in_executor(thread_pool, receive, mq_req)
                        if message:
                            t2 = time.time()
                            bt.logging.debug(f"[LISTEN] Received message in {t2-t1:.4f}s")
                            state = MarketSimulationStateUpdate.parse_dict(message)
                            t3 = time.time()
                            bt.logging.info(f"Parsed state dict ({t3-t2:.4f}s)")
                            response = await self.handle_state(message, state, receive_start)
                            t4 = time.time()
                            bt.logging.debug(f"[LISTEN] handle_state completed in {t4-t3:.4f}s")
                    except Exception as ex:
                        traceback.print_exc()
                        self.pagerduty_alert(f"Exception in posix listener loop : {ex}", details={"trace": traceback.format_exc()})
                    finally:
                        t5 = time.time()
                        bt.logging.debug(f"[LISTEN] Starting respond at {t5:.3f}")
                        await loop.run_in_executor(thread_pool, respond, response)
                        t6 = time.time()
                        bt.logging.debug(f"[LISTEN] Respond completed in {t6-t5:.4f}s")
                        bt.logging.debug(f"[LISTEN] Total loop iteration: {t6-t1:.4f}s")
            finally:
                mq_req.close()
                thread_pool.shutdown(wait=True)

        def listen(self):
            """
            Synchronous wrapper for the asynchronous `_listen` method.
            """
            try:
                os.nice(-10)
            except PermissionError:
                bt.logging.warning("Cannot set process priority (need sudo for negative nice values)")
            try:
                asyncio.run(self._listen())
            except KeyboardInterrupt:
                print("Listening stopped by user.")

        async def orderbook(self, request : Request) -> dict:
            """
            HTTP route endpoint that receives a complete simulator state update over HTTP,
            parses it, and forwards it to `handle_state`.


            This is the HTTP equivalent of the IPC listener used when running a
            distributed or containerized simulator. It performs:
            - Streaming request‑body read.
            - Basic JSON‑structure validation.
            - Construction of a Ypy‑backed state object.
            - Conversion into a typed MarketSimulationStateUpdate model.
            - Delegation to the main validator processing pipeline.


            Args:
            request (Request): Incoming HTTP request containing a JSON‑encoded simulation state update.


            Returns:
            dict: Serialized simulation response batch.
            """
            bt.logging.info("Received state update from simulator.")
            global_start = time.time()
            start = time.time()
            body = bytearray()
            async for chunk in request.stream():
                body.extend(chunk)
            bt.logging.info(f"Retrieved request body ({time.time()-start:.4f}s).")
            if body[-3:].decode() != "]}}":
                raise Exception(f"Incomplete JSON!")
            message = YpyObject(body, 1)
            bt.logging.info(f"Constructed YpyObject ({time.time()-start:.4f}s).")
            state = MarketSimulationStateUpdate.from_ypy(message)
            bt.logging.info(f"Synapse populated ({time.time()-start:.4f}s).")
            del body

            response = await self.handle_state(message, state)

            bt.logging.info(f"State update processed ({time.time()-global_start}s)")
            return response

        async def account(self, request : Request) -> None:
            """
            HTTP route endpoint for receiving event‑level notifications from the simulator
            (e.g., simulation start, simulation end, error reports, market notices).


            Responsibilities:
            - Immediately forward simulation‑start events to miners.
            - Handle simulation‑end markers.
            - Record and persist error‑report batches.
            - Forward all other event notifications to miners.
            - Trigger alerting for msgpack or simulation integrity errors.

            Args:
            request (Request): HTTP request containing a batch of simulator event messages.

            Returns:
            None | dict: `{"continue": True/False}` when error‑report limits are reached.
            Otherwise returns `None`.
            """
            body = bytearray()
            async for chunk in request.stream():
                body.extend(chunk)
            batch = msgspec.json.decode(body)
            bt.logging.info(f"NOTICE : {batch}")
            notices = []
            ended = False
            for message in batch['messages']:
                if message['type'] == 'EVENT_SIMULATION_START':
                    self.onStart(message['timestamp'], FinanceEventNotification.from_json(message).event)
                    continue
                elif message['type'] == 'EVENT_SIMULATION_END':
                    ended = True
                elif message['type'] == 'RESPONSES_ERROR_REPORT':
                    dump_file = self.config.neuron.full_path + f"/{self.last_state.config.simulation_id}.{message['timestamp']}.responses.json"
                    with open(dump_file, "w") as f:
                        json.dump(self.last_response, f, indent=4)
                    error_file = self.config.neuron.full_path + f"/{self.last_state.config.simulation_id}.{message['timestamp']}.error.json"
                    with open(error_file, "w") as f:
                        json.dump(message, f, indent=4)
                    self.msgpack_error_counter += len(message) - 3
                    if self.msgpack_error_counter < 10:
                        self.pagerduty_alert(f"{self.msgpack_error_counter} msgpack deserialization errors encountered in simulator - continuing.", details=message)
                        return { "continue": True }
                    else:
                        self.pagerduty_alert(f"{self.msgpack_error_counter} msgpack deserialization errors encountered in simulator - terminating simulation.", details=message)
                        return { "continue": False }
                notice = FinanceEventNotification.from_json(message)
                if not notice:
                    bt.logging.error(f"Unrecognized notification : {message}")
                else:
                    notices.append(notice)
            await notify(self, notices)
            if ended:
                self.onEnd()

        def cleanup_ipc(self):
            """
            Shuts down the query service and releases all POSIX IPC resources.

            Behavior:
                - Attempts to send a shutdown message to the query service.
                - Waits for graceful termination, falling back to terminate/kill.
                - Closes memory maps and shared memory file descriptors.
                - Closes message queues.
                - Logs detailed warnings for any partial cleanup failures.

            Returns:
                None
            """
            try:
                bt.logging.info("Cleaning up query service...")
                if hasattr(self, 'request_queue'):
                    try:
                        self.request_queue.send(b'shutdown', timeout=1.0)
                        bt.logging.info("Sent shutdown command to query service")
                    except Exception as e:
                        bt.logging.warning(f"Failed to send shutdown command: {e}")
                if hasattr(self, 'query_process') and self.query_process:
                    try:
                        self.query_process.wait(timeout=5.0)
                        bt.logging.info(f"Query service exited with code {self.query_process.returncode}")
                    except subprocess.TimeoutExpired:
                        bt.logging.warning("Query service did not exit gracefully, terminating...")
                        self.query_process.terminate()
                        try:
                            self.query_process.wait(timeout=2.0)
                        except subprocess.TimeoutExpired:
                            bt.logging.error("Query service did not terminate, killing...")
                            self.query_process.kill()

                if hasattr(self, 'request_mem'):
                    try:
                        self.request_mem.close()
                        bt.logging.debug("Closed request memory map")
                    except Exception as e:
                        bt.logging.warning(f"Error closing request memory map: {e}")

                if hasattr(self, 'response_mem'):
                    try:
                        self.response_mem.close()
                        bt.logging.debug("Closed response memory map")
                    except Exception as e:
                        bt.logging.warning(f"Error closing response memory map: {e}")

                if hasattr(self, 'request_shm'):
                    try:
                        self.request_shm.close_fd()
                        bt.logging.debug("Closed request shared memory fd")
                    except Exception as e:
                        bt.logging.warning(f"Error closing request shared memory fd: {e}")

                if hasattr(self, 'response_shm'):
                    try:
                        self.response_shm.close_fd()
                        bt.logging.debug("Closed response shared memory fd")
                    except Exception as e:
                        bt.logging.warning(f"Error closing response shared memory fd: {e}")

                if hasattr(self, 'request_queue'):
                    try:
                        self.request_queue.close()
                        bt.logging.debug("Closed request queue")
                    except Exception as e:
                        bt.logging.warning(f"Error closing request queue: {e}")

                if hasattr(self, 'response_queue'):
                    try:
                        self.response_queue.close()
                        bt.logging.debug("Closed response queue")
                    except Exception as e:
                        bt.logging.warning(f"Error closing response queue: {e}")

                bt.logging.info("Query service cleanup complete")

                bt.logging.info("Cleaning up reporting service...")

                if hasattr(self, 'reporting_request_queue'):
                    try:
                        self.reporting_request_queue.send(b'shutdown', timeout=1.0)
                        bt.logging.info("Sent shutdown command to reporting service")
                    except Exception as e:
                        bt.logging.warning(f"Failed to send shutdown command to reporting: {e}")

                if hasattr(self, 'reporting_process') and self.reporting_process:
                    try:
                        self.reporting_process.wait(timeout=5.0)
                        bt.logging.info(f"Reporting service exited with code {self.reporting_process.returncode}")
                    except subprocess.TimeoutExpired:
                        bt.logging.warning("Reporting service did not exit gracefully, terminating...")
                        self.reporting_process.terminate()
                        try:
                            self.reporting_process.wait(timeout=2.0)
                        except subprocess.TimeoutExpired:
                            bt.logging.error("Reporting service did not terminate, killing...")
                            self.reporting_process.kill()

                if hasattr(self, 'reporting_request_mem'):
                    try:
                        self.reporting_request_mem.close()
                        bt.logging.debug("Closed reporting request memory map")
                    except Exception as e:
                        bt.logging.warning(f"Error closing reporting request memory map: {e}")

                if hasattr(self, 'reporting_response_mem'):
                    try:
                        self.reporting_response_mem.close()
                        bt.logging.debug("Closed reporting response memory map")
                    except Exception as e:
                        bt.logging.warning(f"Error closing reporting response memory map: {e}")

                if hasattr(self, 'reporting_request_shm'):
                    try:
                        self.reporting_request_shm.close_fd()
                        bt.logging.debug("Closed reporting request shared memory fd")
                    except Exception as e:
                        bt.logging.warning(f"Error closing reporting request shared memory fd: {e}")

                if hasattr(self, 'reporting_response_shm'):
                    try:
                        self.reporting_response_shm.close_fd()
                        bt.logging.debug("Closed reporting response shared memory fd")
                    except Exception as e:
                        bt.logging.warning(f"Error closing reporting response shared memory fd: {e}")

                if hasattr(self, 'reporting_request_queue'):
                    try:
                        self.reporting_request_queue.close()
                        bt.logging.debug("Closed reporting request queue")
                    except Exception as e:
                        bt.logging.warning(f"Error closing reporting request queue: {e}")

                if hasattr(self, 'reporting_response_queue'):
                    try:
                        self.reporting_response_queue.close()
                        bt.logging.debug("Closed reporting response queue")
                    except Exception as e:
                        bt.logging.warning(f"Error closing reporting response queue: {e}")

                bt.logging.info("Reporting service cleanup complete")

            except Exception as e:
                bt.logging.error(f"Error during query service cleanup: {e}")
                import traceback
                bt.logging.error(traceback.format_exc())

        def cleanup_executors(self):
            """
            Shuts down thread and process executors used by the validator.

            Executors cleaned:
                - reward_executor (ProcessPoolExecutor)
                - save_state_executor (ThreadPoolExecutor)
                - maintenance_executor (ThreadPoolExecutor)
                - multiprocessing manager (if present)

            Behavior:
                - Each executor is shut down gracefully with wait=True
                - For ProcessPoolExecutor, attempts graceful shutdown first
                - Falls back to immediate termination if graceful fails
                - Logs success or failure for each executor

            Returns:
                None
            """
            if hasattr(self, 'reward_executor') and self.reward_executor is not None:
                try:
                    bt.logging.info("Shutting down reward_executor...")
                    self.reward_executor.shutdown(wait=True, cancel_futures=False)
                    bt.logging.info("reward_executor shut down successfully")
                except Exception as ex:
                    bt.logging.error(f"Error shutting down reward_executor: {ex}")
                    try:
                        bt.logging.warning("Attempting to terminate reward_executor processes...")
                        for process in self.reward_executor._processes.values():
                            if process.is_alive():
                                process.terminate()
                                process.join(timeout=2.0)
                                if process.is_alive():
                                    process.kill()
                        bt.logging.info("reward_executor processes terminated")
                    except Exception as term_ex:
                        bt.logging.error(f"Error terminating reward_executor: {term_ex}")

            thread_executors = {
                'save_state_executor': getattr(self, 'save_state_executor', None),
                'maintenance_executor': getattr(self, 'maintenance_executor', None),
            }

            for name, executor in thread_executors.items():
                if executor is not None:
                    try:
                        bt.logging.info(f"Shutting down {name}...")
                        executor.shutdown(wait=True, cancel_futures=False)
                        bt.logging.info(f"{name} shut down successfully")
                    except Exception as ex:
                        bt.logging.error(f"Error shutting down {name}: {ex}")

            if hasattr(self, 'manager'):
                try:
                    bt.logging.info("Shutting down multiprocessing manager...")
                    self.manager.shutdown()
                    bt.logging.info("Manager shut down successfully")
                except Exception as ex:
                    bt.logging.error(f"Error shutting down manager: {ex}")

            bt.logging.info("Executor cleanup complete")

        def cleanup_event_loop(self):
            """
            Gracefully shuts down the main event loop and any pending tasks.

            Behavior:
                - Cancels all pending tasks in the main loop
                - Waits for task cancellation to complete
                - Stops the event loop if still running
                - Closes the event loop

            Returns:
                None
            """
            try:
                if hasattr(self, 'main_loop') and self.main_loop and not self.main_loop.is_closed():
                    bt.logging.info("Shutting down main event loop...")

                    pending = asyncio.all_tasks(self.main_loop)
                    if pending:
                        bt.logging.info(f"Cancelling {len(pending)} pending tasks...")
                        for task in pending:
                            task.cancel()

                        self.main_loop.run_until_complete(
                            asyncio.gather(*pending, return_exceptions=True)
                        )

                    if self.main_loop.is_running():
                        self.main_loop.stop()

                    self.main_loop.close()
                    bt.logging.info("Main event loop shut down successfully")
            except Exception as ex:
                bt.logging.error(f"Error shutting down main event loop: {ex}")
                bt.logging.error(traceback.format_exc())

        def cleanup(self):
            """
            Performs full resource cleanup for the validator during shutdown.
            """
            if self._cleanup_done:
                bt.logging.debug("Cleanup already completed, skipping")
                return

            bt.logging.info("Starting validator cleanup...")
            self._cleanup_done = True

            try:
                bt.logging.info("Waiting for active operations to complete...")
                wait_timeout = 30.0
                wait_start = time.time()

                while (self.shared_state_rewarding or
                    self.shared_state_saving or
                    self.shared_state_reporting or
                    self.maintaining or
                    self.compressing or
                    self.querying):

                    elapsed = time.time() - wait_start
                    if elapsed > wait_timeout:
                        bt.logging.warning(
                            f"Timeout waiting for operations after {elapsed:.2f}s"
                        )
                        break
                    time.sleep(0.1)

                self.cleanup_executors()
                self.cleanup_ipc()
                self.cleanup_event_loop()

                bt.logging.success("Validator cleanup completed successfully")

            except Exception as ex:
                bt.logging.error(f"Error during cleanup: {ex}")
                bt.logging.error(traceback.format_exc())


if __name__ == "__main__":
    from taos.im.validator.update import check_repo, update_validator, check_simulator, rebuild_simulator, restart_simulator
    from taos.im.validator.forward import forward, notify
    from taos.im.validator.reward import get_rewards

    if float(platform.freedesktop_os_release()['VERSION_ID']) < 22.04:
        raise Exception(f"taos validator requires Ubuntu >= 22.04!")

    bt.logging.info("Initializing validator...")
    app = FastAPI()
    validator = Validator()
    try:
        app.include_router(validator.router)

        bt.logging.info("Starting background threads...")
        threads = []
        for name, target in [('Seed', validator.seed), ('Monitor', validator.monitor), ('Listen', validator.listen)]:
            try:
                bt.logging.info(f"Starting {name} thread...")
                thread = Thread(target=target, daemon=True, name=name)
                thread.start()
                threads.append(thread)
            except Exception as ex:
                validator.pagerduty_alert(f"Exception starting {name} thread: {ex}", details={"trace" : traceback.format_exc()})
                raise

        time.sleep(1)
        for thread in threads:
            if not thread.is_alive():
                validator.pagerduty_alert(f"Failed to start {thread.name} thread!")
                raise RuntimeError(f"Thread '{thread.name}' failed to start")

        bt.logging.info(f"All threads running. Starting FastAPI server and main event loop...")

        def run_main_loop():
            """Run the pre-created main event loop."""
            async def keep_alive():
                bt.logging.info(f"Main event loop started for background tasks")
                bt.logging.debug(f"[MAINLOOP] Thread: {threading.current_thread().name}")
                bt.logging.debug(f"[MAINLOOP] Loop: {id(validator.main_loop)}")
                try:
                    while True:
                        await asyncio.sleep(1)
                except KeyboardInterrupt:
                    bt.logging.info("Main event loop stopping...")
            loop = validator.main_loop
            asyncio.set_event_loop(loop)
            validator._main_loop_ready.set()
            bt.logging.debug(f"[MAINLOOP] Running loop: {id(loop)}")
            try:
                loop.run_until_complete(keep_alive())
            finally:
                loop.close()

        main_loop_thread = Thread(target=run_main_loop, daemon=True, name='main')
        main_loop_thread.start()
        threads.append(main_loop_thread)
        time.sleep(0.5)
        bt.logging.info(f"Starting FastAPI server on port {validator.config.port}...")
        uvicorn.run(app, host="0.0.0.0", port=validator.config.port)
    except KeyboardInterrupt:
        bt.logging.info("Keyboard interrupt received")
    except Exception as ex:
        bt.logging.error(f"Fatal error: {ex}")
        bt.logging.debug(traceback.format_exc())
        sys.exit(1)